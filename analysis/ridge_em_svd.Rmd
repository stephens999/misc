---
title: "ridge_em_svd"
author: "Matthew Stephens"
date: "2020-06-26"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

Following up on these [EM algorithms to fit Ridge by EB](ridge_em.html), 
I look at implementing these kinds of ideas when an SVD for $X=UDV'$ is available
(or, simply by doing SVD of $X$ as a pre-computation step). Note that 
randomized methods can allow very fast approximation
of the SVD of $X$ for large matrices, and I have in mind we may be able to exploit
these down the line, especially as we may only need approximate solutions to ridge regression for
our purposes.

I assume we are in the big $p$ regime, so $D$ is $k$ \times $k$ with $k<p$, and $V'V = I_k$, and $U'U=I_k$. Often we will have $k=n$, in which case $UU'= U'U = I_n$. 


The model is: 
$$Y \sim N(Xb, s^2I_n)$$

Premultiplying by $U'$ gives:
$$U'Y \sim N(DV'b, s^2 I_k)$$
which we can write as
$$\tilde{Y}_j \sim N(\theta_j, s^2)$$
$$\theta_j \sim N(0, s_b^2 d_j^2)$$.

And we can solve this by EM, just as before. Of course we can parameterize
in various ways. 

Some derivations are [here](EM_Ridge_SVD.pdf).

Here is the EM for the simple parameterization as above:
```{r}
ridge_indep_em1 = function(y, d2, s2, sb2, niter=10){
  k = length(y)
  loglik = rep(0,niter)
  
  for(i in 1:niter){
    
    prior_var = sb2*d2
    data_var = s2
    
    loglik[i] = sum(dnorm(y,mean=0,sd = sqrt(sb2*d2 + s2),log=TRUE))
    
    # update sb2
    post_var = 1/((1/prior_var) + (1/data_var)) #posterior variance of theta
    post_mean =  post_var * (1/data_var) * y # posterior mean of theta
    sb2 = mean((post_mean^2 + post_var)/d2)
     
    # update s2
    r = y - post_mean # residuals
    s2 = mean(r^2 + post_var)
  }
  return(list(s2=s2,sb2=sb2,loglik=loglik,postmean = post_mean))
}
```


## Scaled parameterization

Here we take the $s_b$ out of the prior on $\theta_j$:
$$y_j \sim N(s_b \theta_j, s^2)$$
$$\theta_j \sim N(0,d_j^2).$$

Note that we could also put the $d_j$ into the mean of $y_j$ and have $\theta_j \sim N(0,1)$
but this ends up leading to exactly the same EM algorithm. (In earlier versions of
this document I implemented this, but it turned out to indeed be identical, so I removed it.)

Note also that here I give the option to recompute quantities between updates of `sb2` and
`s2`. However, this didn't help in any examples I tried (not shown).

```{r}
ridge_indep_em2 = function(y, d2, s2, sb2, niter=10, recompute_between_updates = FALSE){
  k = length(y)
  loglik = rep(0,niter)
  for(i in 1:niter){
    loglik[i] = sum(dnorm(y,mean=0,sd = sqrt(sb2*d2 + s2),log=TRUE))
    
    prior_var = d2 # prior variance for theta
    data_var = s2/sb2 # variance of y/sb, which has mean theta
    post_var = 1/((1/prior_var) + (1/data_var)) #posterior variance of theta
    post_mean =  post_var * (1/data_var) * (y/sqrt(sb2)) # posterior mean of theta
    
    sb2 = (sum(y*post_mean)/sum(post_mean^2 + post_var))^2
    
    if(recompute_between_updates){
      prior_var = d2 # prior variance for theta
      data_var = s2/sb2 # variance of y/sb, which has mean theta
      post_var = 1/((1/prior_var) + (1/data_var)) #posterior variance of theta
      post_mean =  post_var * (1/data_var) * (y/sqrt(sb2)) # posterior mean of theta
    }
    
    r = y - sqrt(sb2) * post_mean # residuals
    s2 = mean(r^2 + sb2 * post_var)
    
  }
  return(list(s2=s2,sb2=sb2,loglik=loglik,postmean = sqrt(sb2) * post_mean))
}

```


## Hybrid/redundant parameterization

As [before](ridge_em.html) we take a hybrid approach aimed
at getting the best of both worlds.

$$y_j \sim N(s_b \theta_j, s^2)$$
$$\theta_j \sim N(0,l^2 d_j^2).$$
The updates involve combinations of the updates in em1 and em2.

```{r}
ridge_indep_em3 = function(y, d2, s2, sb2, l2,niter=10){
  k = length(y)
  loglik = rep(0,niter)
  for(i in 1:niter){
    loglik[i] = sum(dnorm(y,mean=0,sd = sqrt(sb2*l2*d2 + s2),log=TRUE))
    
    prior_var = d2*l2 # prior variance for theta
    data_var = s2/sb2 # variance of y/sb, which has mean theta
    post_var = 1/((1/prior_var) + (1/data_var)) #posterior variance of theta
    post_mean =  post_var * (1/data_var) * (y/sqrt(sb2)) # posterior mean of theta
    
    sb2 = (sum(y*post_mean)/sum(post_mean^2 + post_var))^2
    l2 = mean((post_mean^2 + post_var)/d2)
      
    r = y - sqrt(sb2) * post_mean # residuals
    s2 = mean(r^2 + sb2 * post_var)
    
  }
  return(list(s2=s2,sb2=sb2,loglik=loglik,postmean = sqrt(sb2) *post_mean))
}

```


## Simple simulation

Here we try a simple simulation to test:
```{r}
set.seed(100)
sd = 1
n = 100
p = n
X = matrix(rnorm(n*p),ncol=n)
btrue = rnorm(n)
y = X %*% btrue + sd*rnorm(n)

plot(X %*% btrue, y)
```


Here I define a function to plot the log-likelihoods:

```{r}
plot_loglik = function(res){
  maxloglik = max(res[[1]]$loglik)
  minloglik = min(res[[1]]$loglik)
  maxlen =length(res[[1]]$loglik)
  for(i in 2:length(res)){
    maxloglik = max(c(maxloglik,res[[i]]$loglik))
    minloglik = min(c(minloglik,res[[i]]$loglik))
    maxlen= max(maxlen, length(res[[i]]$loglik))
  }
  
  
  plot(res[[1]]$loglik,type="n",ylim=c(minloglik,maxloglik),xlim=c(0,maxlen),ylab="log-likelihood",
       xlab="iteration")
  for(i in 1:length(res)){
    lines(res[[i]]$loglik,col=i,lwd=2)
  }

}
```

Run all the methods: the scaled parameterization is worst here:
```{r}
X.svd = svd(X)
ytilde = drop(t(X.svd$u) %*% y)
yt.em1 = ridge_indep_em1(ytilde,X.svd$d^2,1,1,100)
yt.em2 = ridge_indep_em2(ytilde,X.svd$d^2,1,1,100)
yt.em3= ridge_indep_em3(ytilde,X.svd$d^2,1,1,1,100)

plot_loglik(list(yt.em1,yt.em2,yt.em3))
```

Check that the posterior means are all the same
```{r}
plot(ytilde, yt.em1$postmean,col=1)
points(ytilde, yt.em2$postmean,col=2)
points(ytilde, yt.em3$postmean,col=3)
abline(a=0,b=1)
```


Try different initializations. Here `s2=.1` and `sb2=10`. 
```{r}
yt.em1 = ridge_indep_em1(ytilde,X.svd$d^2,.1,10,100)
yt.em2 = ridge_indep_em2(ytilde,X.svd$d^2,.1,10,100)
yt.em3= ridge_indep_em3(ytilde,X.svd$d^2,.1,10,1,100)

plot_loglik(list(yt.em1,yt.em2,yt.em3))
```


Here `s2=10` and `sb2=.1`.
```{r}
yt.em1 = ridge_indep_em1(ytilde,X.svd$d^2,10,.1,50)
yt.em2 = ridge_indep_em2(ytilde,X.svd$d^2,10,.1,50)
yt.em3= ridge_indep_em3(ytilde,X.svd$d^2,10,.1,1,50)

plot_loglik(list(yt.em1,yt.em2,yt.em3))
```


## No signal

This simulation has no signal (b=0). Methods are similar here. 

```{r}
btrue = rep(0,n)
y = X %*% btrue + sd*rnorm(n)

X.svd = svd(X)
ytilde = drop(t(X.svd$u) %*% y)
yt.em1 = ridge_indep_em1(ytilde,X.svd$d^2,1,1,100)
yt.em2 = ridge_indep_em2(ytilde,X.svd$d^2,1,1,100)
yt.em3 = ridge_indep_em3(ytilde,X.svd$d^2,1,1,1,100)


plot_loglik(list(yt.em1,yt.em2,yt.em3))
```



## Trendfiltering Simulations

This is more challenging example (in that the design matrix is correlated)

### High Signal

```{r}
set.seed(100)
sd = 1
n = 100
p = n
X = matrix(0,nrow=n,ncol=n)
for(i in 1:n){
  X[i:n,i] = 1:(n-i+1)
}
btrue = rep(0,n)
btrue[40] = 8
btrue[41] = -8
y = X %*% btrue + sd*rnorm(n)

plot(y)
lines(X %*% btrue)
```

Run the methods: there is a clear advantage of simple parameterization.

```{r}
X.svd = svd(X)
ytilde = drop(t(X.svd$u) %*% y)

yt.em1 = ridge_indep_em1(ytilde,X.svd$d^2,1,1,100)
yt.em2 = ridge_indep_em2(ytilde,X.svd$d^2,1,1,100)
yt.em3 = ridge_indep_em3(ytilde,X.svd$d^2,1,1,1,100)

plot_loglik(list(yt.em1,yt.em2,yt.em3))

```

Fits are different:
```{r}
plot(y)
lines(X %*% btrue,col="gray")
lines(X.svd$u %*% yt.em1$postmean,lwd=2)
lines(X.svd$u %*% yt.em2$postmean,col=2,lwd=2)
lines(X.svd$u %*% yt.em3$postmean,col=3,lwd=2)
```

### No signal case

Try no signal case
```{r}
sd = 1
n = 100
p = n
X = matrix(0,nrow=n,ncol=n)
for(i in 1:n){
  X[i:n,i] = 1:(n-i+1)
}
btrue = rep(0,n)

y = X %*% btrue + sd*rnorm(n)

plot(y)
lines(X %*% btrue)
```


Run the EM: there is a clear advantage of scaled parameterizations.
```{r}
X.svd = svd(X)
ytilde = drop(t(X.svd$u) %*% y)

yt.em1 = ridge_indep_em1(ytilde,X.svd$d^2,1,1,100)
yt.em2 = ridge_indep_em2(ytilde,X.svd$d^2,1,1,100)
yt.em3 = ridge_indep_em3(ytilde,X.svd$d^2,1,1,1,100)

plot_loglik(list(yt.em1,yt.em2,yt.em3))
```
