---
title: "mr_ash_pen"
author: "Matthew Stephens"
date: "2020-05-15"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

The idea here is to code up mr.ash as a penalized method, compute gradients etc.

The penalized version of the mr.ash problem is (see Theorem 6 in mr.ash draft)
$$\min_b (1/2\sigma^2) ||y - Xb||_2 + (1/\sigma^2) \sum_j \rho_{g,s_j}(b_j) + 0.5(n-p) \log(2\pi \sigma^2)$$
Where the penalty $\rho$ depends on the prior ($g$) and the $s_j^2:=\sigma^2/(x_j'x_j)$. Here I'm going to assume for simplicity
that $g$ denotes the prior on b, ie not scale the prior on b by the residual variance as in the paper.

The penalty $\rho_{g,s_j}(b)$ is inconvenient to compute because it involves the inverse of $S$ (the posterior mean shrinkage function) which is not analytically available to us,
at least in our current state of knowledge.

A simple idea is to rewrite the problem as:
$$\min_b (1/2\sigma^2) ||y - XS_{g,s}(b)||_2 + (1/\sigma^2) \rho_{g,s}(S_{g,s}(b)) + 0.5(n-p) \log(2\pi \sigma^2)$$
Here $S_{g,s}(b)$ means apply $S$ element-wise to the vector $b$.
There is a slight abuse of notation here as $s_j$ may vary across $j$.
To be explicit, the $j$th element is $S_{g,s}(b)$ is equal to $S_{g,s_j}(b_j)$.


Because $S_{g,s}$ is invertible there is no loss of generality in writing the optimization this way. Furthermore, $$h_{g,s}(b):=\rho_{g,s}(S_{g,s}(b))$$ is easy to compute. Indeed
$$h_{g,s}(b) = -s^2 l_{g,s}(b) - 0.5 s^4 [l_{g,s}'(b)]^2$$

where $l_{g,s}(b)$ is the marginal log-likelihood function under the normal means model with prior $g$ and variance $s$.
That is $l_{g,s}(b) = \log(f(b))$
where
$$f(b) := \sum_k \pi_k N(b; 0, \sigma_k^2+s^2).$$

Note: Tweedie's formula says that relates shrinkage in the normal means model $S$ to the marginal loglikelihood $l$, via
$$S_{f,s^2}(b) = b + s^2 l'(b)$$. 
So the term $s^4 [l'(b)]^2$ above is $(S(b)-b)^2$.



## Summary of optimization problem

I wanted to give a succinct summary of the overall optimization
problem to share with others.

For fixed $\sigma^2$, and letting $w$ represent the mixture
proportions in the prior $g$ (which we often denote $\pi$) 
we can write it as follows.

Given $y \in R^n$ and $X \in R^{n \times p}$,
$$\min ||y - XS(b; w)||_2 + h(b,w)$$
subject to: $w \in S_K$ the simplex of dimension $K$, and $b \in R^p$.

$$h(b,w) = -s^2 l_{w,s}(b) - 0.5 s^4 [l_{w,s}'(b)]^2$$


## Code for fundamental functions

Everything can be written in terms of the marginal likelihood $f$ and its first 
two derivatives, so we code those up first. I have not been careful about numerical
issues here.. will need to deal with those at some point.

```{r}
#y,s are vectors of length n
#w, sigma are vectors of length K (w are prior mixture proportions)
# returns an n-vector of "marginal likelihoods" under mixture prior
f = function(b, s, w, sigma){
  if(length(s)==1){s = rep(s,length(b))}
  sigmamat   <- outer(s^2, sigma^2, `+`) # n time k
  llik_mat   <- -0.5 * (log(sigmamat) + b^2 / sigmamat)
  #llik_norms <- apply(llik_mat, 1, max)
  #L_mat      <- exp(llik_mat - llik_norms)
  L_mat <- exp(llik_mat)
  return((1/sqrt(2*pi)) * as.vector(colSums(w * t(L_mat))))
}

#returns a vector of the derivative of f evaluated at each element of y
f_deriv = function(b, s, w, sigma){
  if(length(s)==1){s = rep(s,length(b))}
  sigmamat   <- outer(s^2, sigma^2, `+`) # n time k
  llik_mat   <- -(3/2) * log(sigmamat) -0.5* b^2 / sigmamat
  #llik_norms <- apply(llik_mat, 1, max)
  #L_mat      <- exp(llik_mat - llik_norms)
  L_mat <- exp(llik_mat)
  return((-b/sqrt(2*pi)) * as.vector(colSums(w * t(L_mat))))
}

# returns f_deriv/b ok even if b=0
f_deriv_over_b = function(b, s, w, sigma){
  if(length(s)==1){s = rep(s,length(b))}
  sigmamat   <- outer(s^2, sigma^2, `+`) # n time k
  llik_mat   <- -(3/2) * log(sigmamat) -0.5* b^2 / sigmamat
  #llik_norms <- apply(llik_mat, 1, max)
  #L_mat      <- exp(llik_mat - llik_norms)
  L_mat <- exp(llik_mat)
  return((-1/sqrt(2*pi)) * as.vector(colSums(w * t(L_mat))))
}

#returns a vector of the second derivatives of f evaluated at each element of y
f_deriv2 = function(b, s, w, sigma){
  if(length(s)==1){s = rep(s,length(b))}
  sigmamat   <- outer(s^2, sigma^2, `+`) # n time k
  llik_mat   <- -(5/2) * log(sigmamat) -0.5* b^2 / sigmamat
  #llik_norms <- apply(llik_mat, 1, max)
  #L_mat      <- exp(llik_mat - llik_norms)
  L_mat <- exp(llik_mat)
  return((b^2/sqrt(2*pi)) * as.vector(colSums(w * t(L_mat)))+ f_deriv_over_b(b,s,w,sigma))
}
```

Check the derivative code numerically:
```{r}
n = 100
k = 5
b = rnorm(n)
w = rep(1/k,k)
prior_grid = c(0,1,2,3,4,5)
eps=1e-5

plot((f(b+eps,1,w,prior_grid)-f(b,1,w,prior_grid))/eps, f_deriv(b,1,w,prior_grid),xlab="numerical 1st derivative", ylab="analytic 1st derivative")
abline(a=0,b=1)

plot((f_deriv(b+eps,1,w,prior_grid)-f_deriv(b,1,w,prior_grid))/eps, f_deriv2(b,1,w,prior_grid), xlab="numerical 2nd derivative", ylab="analytic 2nd derivative")
abline(a=0,b=1)
```

Now we have $$l(b) = \log f(b)$$, 
$$l'(b) = f'(b)/f(b)$$, $$l''(b) = (f(b)f''(b)-f'(b)^2)/f(b)^2$$,


```{r}
l = function(b, s, w, prior_grid){
  log(f(b,s,w,prior_grid))
}

l_deriv = function(b, s, w, prior_grid){
  f_deriv(b,s,w,prior_grid)/f(b,s,w,prior_grid)
}

l_deriv2 = function(b, s, w, prior_grid){
  ((f_deriv2(b,s,w,prior_grid)*f(b,s,w,prior_grid))-f_deriv(b,s,w,prior_grid)^2)/f(b,s,w,prior_grid)^2
}

plot((l_deriv(b+eps,1,w,prior_grid)-l_deriv(b,1,w,prior_grid))/eps, l_deriv2(b,1,w,prior_grid), xlab="numerical 2nd derivative", ylab="analytic 2nd derivative")
abline(a=0,b=1)

```

And with these in place we can compute the shrinkage function and penalty function $h$, using
$$h(b) = -l(b) - 0.5(l'(b))^2$$
$$S(b) = b+ s^2l'(b)$$
$$S'(b) = 1 + s^2 l''(b)$$.

```{r}
S = function(b, s, w, prior_grid){
  return(b + s^2 * l_deriv(b,s,w,prior_grid))
}
S_deriv = function(b, s, w, prior_grid){
  return(1+ s^2 * l_deriv2(b,s,w,prior_grid))
}
h = function(b,s,w,prior_grid){
  return(-s^2 * l(b,s,w,prior_grid) - 0.5 * s^4 * l_deriv(b,s,w,prior_grid)^2)
}
h_deriv = function(b,s,w,prior_grid){
  return(-s^2 * l_deriv(b,s,w,prior_grid) - s^4 * l_deriv(b,s,w,prior_grid) * l_deriv2(b,s,w,prior_grid))
}


plot((h(b+eps,1,w,prior_grid)-h(b,1,w,prior_grid))/eps, h_deriv(b,1,w,prior_grid), xlab="numerical 1st derivative", ylab="analytic 1st derivative")
abline(a=0,b=1)
```


## Inverting S by Newton-Raphson method

One thing we might want to do is invert S.
We don't have analytic form for this, but we can use the Newton-Raphson  method
to solve $S(x)=y$. The iterates would be 
$$x \leftarrow x - (S(x)-y)/S'(x)  $$


```{r}
y = seq(-10,10,length=100)
x=y 
par(mfrow=c(5,5))
par(mar=rep(1.5,4))
w = rep(0.5,2)
prior_grid = c(0,5)
for(i in 1:25){
  plot(S(x,1,w,prior_grid),y)
  x = x-(S(x,1,w,prior_grid)-y)/S_deriv(x,1,w,prior_grid)
}
plot(S(x,1,w,prior_grid),y)
plot(x,y)
```



## Optimizing the objective

```{r}
obj = function(b, y, X, residual_var, w, prior_grid){
  d = colSums(X^2)
  s = sqrt(residual_var/d)
  r = y - X %*% S(b,s,w,prior_grid)
  return(0.5*(1/residual_var)*sum(r^2) + (1/residual_var)*sum(h(b,s,w,prior_grid)))
}

obj_grad = function(b, y, X, residual_var, w, prior_grid){
  d = colSums(X^2)
  s = sqrt(residual_var/d)
  r = y - X %*% S(b,s,w,prior_grid)
  return((-1/residual_var)*(t(r) %*% X) * S_deriv(b,s,w,prior_grid) + (1/residual_var)* h_deriv(b,s,w,prior_grid))
}

```

Try it out on simple simulation:
```{r}
n = 100
p = 20
X = matrix(rnorm(n*p),nrow=n)
norm = colSums(X^2)
X = t(t(X)/sqrt(norm))
b = rnorm(p)
y = X %*% b + rnorm(n)

(obj(b,y,X,1,w,prior_grid)-obj(b+c(eps,rep(0,p-1)),y,X,1,w,prior_grid))/eps
-obj_grad(b,y,X,1,w,prior_grid)[1]
```



```{r}
b.cg.warm = optim(b,obj,gr=obj_grad,method="CG",y=y,X=X,residual_var=1, w=w,prior_grid=prior_grid)
b.cg.null = optim(rep(0,p),obj,gr=obj_grad,method="CG",y=y,X=X,residual_var=1, w=w,prior_grid=prior_grid)
b.bfgs.warm = optim(b,obj,gr=obj_grad,method="BFGS",y=y,X=X,residual_var=1, w=w,prior_grid=prior_grid)
b.bfgs.null = optim(rep(0,p),obj,gr=obj_grad,method="BFGS",y=y,X=X,residual_var=1, w=w,prior_grid=prior_grid)


b.cg.warm$value
b.cg.null$value
b.bfgs.warm$value
b.bfgs.null$value

plot(S(b.cg.warm$par,1,w,prior_grid),b)
```

Some comments from Mihai: BFGS stores a dense approximation to the Hessian,
so won't be good for big problems. However, "limited"-BFGS might work (L-BFGS?).
CG needs preconditioning in general; BFGS does not because it is computing
an approximation to the Hessian.

Thoughts from me: maybe we can compute the Hessian directly and efficiently when X
is, say, the trend filtering matrix. 

Another question: if we write h as a function of pi, is h(pi) convex? Is rho(pi) convex?



# Trendfiltering

Here I try a really hard trend-filtering example.
```{r}
set.seed(100)
sd = 1
n = 100
p = n
X = matrix(0,nrow=n,ncol=n)
for(i in 1:n){
  X[i:n,i] = 1:(n-i+1)
}
btrue = rep(0,n)
btrue[40] = 8
btrue[41] = -8

Y = X %*% btrue + sd*rnorm(n)

```


```{r}
norm = colSums(X^2)
X = t(t(X)/sqrt(norm))
btrue = btrue * sqrt(norm)
plot(Y)
lines(X %*% btrue)
```


```{r}
prior_grid=c(0,10,100,1000)
w=rep(1/4,4)
b.cg.null = optim(rep(0,p),obj,gr=obj_grad,method="CG",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid,control=list(maxit=1000))
b.bfgs.null = optim(rep(0,p),obj,gr=obj_grad,method="BFGS",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid,control=list(maxit=1000))

plot(Y)
lines(X %*% S(b.cg.null$par,1,w,prior_grid))
lines(X %*%  S(b.bfgs.null$par,1,w,prior_grid))


b.cg.warm = optim(btrue,obj,gr=obj_grad,method="CG",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid)
b.bfgs.warm = optim(btrue,obj,gr=obj_grad,method="BFGS",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid)
lines(X %*% S(b.cg.warm$par,1,w,prior_grid))
lines(X %*% S(b.bfgs.warm$par,1,w,prior_grid),col=2)

b.cg.warm$value
b.cg.null$value
b.bfgs.warm$value
b.bfgs.null$value
```


This case is ridge regression, so should be convex... try it out.
```{r}
prior_grid=c(10,10)
w=rep(1/2,2)
b.cg.null = optim(rep(0,p),obj,gr=obj_grad,method="CG",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid,control=list(maxit=1000))
b.bfgs.null = optim(rep(0,p),obj,gr=obj_grad,method="BFGS",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid,control=list(maxit=1000))

plot(Y)
lines(X %*% S(b.cg.null$par,1,w,prior_grid))
lines(X %*%  S(b.bfgs.null$par,1,w,prior_grid))

b.bfgs.null$value
b.cg.null$value


```


Try a prior with more overlapping components
```{r}
prior_grid=seq(0,100,length=100)
w=rep(1/100,100)
b.cg.null = optim(rep(0,p),obj,gr=obj_grad,method="CG",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid,control=list(maxit=1000))
b.bfgs.null = optim(rep(0,p),obj,gr=obj_grad,method="BFGS",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid,control=list(maxit=1000))

plot(Y)
lines(X %*% S(b.cg.null$par,1,w,prior_grid))
lines(X %*%  S(b.bfgs.null$par,1,w,prior_grid))

b.bfgs.null$value
b.cg.null$value


```

And now revert to the sparser prior
```{r}
prior_grid=c(0,10,100,1000)
w=rep(1/4,4)
b.cg.null = optim(b.cg.null$par,obj,gr=obj_grad,method="CG",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid,control=list(maxit=1000))
b.bfgs.null = optim(b.bfgs.null$par,obj,gr=obj_grad,method="BFGS",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid,control=list(maxit=1000))
plot(Y)
lines(X %*% S(b.cg.null$par,1,w,prior_grid))
lines(X %*%  S(b.bfgs.null$par,1,w,prior_grid))


b.bfgs.null$value
b.cg.null$value

```


Try initializing from bhat the "overfit" solution.

```{r}
bhat = chol2inv(chol(t(X) %*% X)) %*% t(X) %*% Y
plot(X %*% bhat)
lines(Y)
b.bfgs.bhat = optim(bhat,obj,gr=obj_grad,method="BFGS",y=Y,X=X,residual_var=1, w=w,prior_grid=prior_grid,control=list(maxit=10000))
b.bfgs.bhat$value
plot(Y)
lines(X %*%  S(as.vector(b.bfgs.null$par),1,w,prior_grid))
lines(X %*%  S(as.vector(b.bfgs.warm$par),1,w,prior_grid),col=2)
lines(X %*%  S(as.vector(b.bfgs.bhat$par),1,w,prior_grid),col=3)
```

