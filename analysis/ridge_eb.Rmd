---
title: "ridge_eb"
author: "Matthew Stephens"
date: "2020-05-20"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

My goal here is to develop some methods for doing EB analysis for ridge regression.

The model is
$$y = Xb + e$$
where $y,e$ are $n-$vectors, $b$ a $p$-vector and $X$ an $n \times p$ matrix.
Let $s$ denote the residual standard error, so
$$e \sim N(0,s^2 I_n).$$

As a reminder, assuming $X'X$ is invertible, the OLS estimator $\hat{b}=(X'X)^{-1}X'y$ satisfies
$$\hat{b} \sim N(b, s^2 (X'X)^{-1})$$

Assume the prior is
$$b \sim N(s\mu 1_p,s_b^2 s^2 I_p)$$ 
where $1_p$ is the  $p$-vector of all ones and $\mu$ is a scalar. (So we allow $b$ to have non-zero mean, for reasons I won't go into here, and we scale the prior by $s$.) 


## Marginal likelihood

Putting these together we have 
$$ y | \mu, s_b^2, s^2 \sim N(s\mu X1_p, s^2(s_b^2XX'+I_n))$$

Thus, marginalizing out $b$, the log-likelihood for $\mu, s_b,s$ is
$$l(\mu,s_b,s) = -0.5 \log(|2\pi \Sigma|) - 0.5 (y-s\mu x_+)'\Sigma^{-1}(y-s\mu x_+)$$
where 
$$\Sigma = s^2(s_b^2 XX' + I_n)$$
and $x_+ := X1_p$ are the row-sums of $X$.


## Estimating hyperparameters

Here we have changed from mean $s\mu$ to $\mu$...

If $Y=Xb+E$ and $X=UDV'$ then
$$T:=D^{-1}U'Y \sim N(\mu V'1_p, s^2(s_b^2I+D^{-2})).$$
And we can do inference for $\mu,s,s_b$ under this transformed model.

It is easy to show the mle for $s$ is:
$$\hat{s^2} = (1/k) \sum_j w_j(T_j-\mu z_j)^2$$
where $w_j=1/(s^2_b+d_j^{-2})$ are the invere-variance weights,
$k$ is the length of $T$ and $z_j = (V'1_p)_j$.

And the mle for $\mu$ is 
$$\hat{\mu} = \sum_j w_j T_j z_j / \sum_j z_j^2 w_j$$.


```{r}
set.seed(1)
n = 100
p = 100
s = 1
sb = 1
mu= .1

X = matrix(0,nrow=n,ncol=n)
for(i in 1:n){
  X[i:n,i] = 1 #1:(n-i+1)
}
btrue = mu+rnorm(n,sd=sb*s)
#X = scale(X)
#t(t(X)/sqrt(colSums(X^2)))

Y = X %*% btrue + rnorm(n)
plot(Y)
lines(X %*% btrue)

#X = matrix(rnorm(n*p),nrow=n,ncol=p)

X.svd = svd(X)

# fit the model X_j \sim  N(mu z_j,s^2(sb^2+shat^2_j))
# estimate sb (from which mle's of mu and s2 are easy)
# it is like ebnm with N(0,s^2 sb^2) prior and s^2 shat^2_j error
# but with s^2 unknown
# x is vector of observations
# shat is vector of corresponding standard errors (up to constant s)

# this is negative log-likelihood evaluated at mle for s,mu
# z is V't
# par is (log(sb),mu) which are both scalars


# set mu=0 or s2 to fix it
compute_w_mu_s2 = function(sb,x,shat,z,mu=NULL,s2 = NULL){
  w = 1/(sb^2 + shat^2) # inverse variance weights
  if(is.null(mu)){
    mu = sum(w*z*x)/sum(z^2*w) # this is mle of mu
  }
  if(is.null(s2)){
    s2 = mean(w*(x-mu*z)^2) #this is the mle of s2
  }
  return(list(w=w,mu=mu,s2=s2))
}

#set mu=0 to fix it
neg_loglik = function(par,x,shat,z,mu=NULL, s2=NULL){
    sb= exp(par[1])
    aux = compute_w_mu_s2(sb,x,shat,z,mu,s2)
    var_x = aux$s2/aux$w
    return(0.5 *(sum(log(aux$s2/aux$w) + aux$w*(x-aux$mu*z)^2/aux$s2)))
}


TT = (t(X.svd$u) %*% Y)/X.svd$d # what i called T above
z = colSums(X.svd$v)
shat = 1/X.svd$d
lsb = seq(-10,10,length=100)
ll = rep(0,100)
for(i in 1:length(lsb)){
  ll[i] = neg_loglik(lsb[i],TT,shat,z)
}
plot(lsb,ll)

optim(par=0,fn=neg_loglik,x=TT,shat=shat,z=z)
optim(par=0,fn=neg_loglik,x=TT,shat=shat,z=z,method="Brent",lower=-5,upper=5)


# # this is wrong.. it is partial derivative with respect to sb, keeping s fixed
# # need to compute derivative properly.... 
# neg_loglik_grad = function(sb,x,xhat){
#   var_x_over_s = sb^2 + shat^2
#   s2 = mean(x^2/var_x_over_s) #this is the mle
#   var_x = s2 * var_x_over_s
#   return(0.5* sum(1/var_x_over_s - x^2/var_x))
# }
# neg_loglik_grad(0.1,xx,shat)
# eps=1e-5
# (log_lik(0.1,xx,shat) - log_lik(0.1-eps,xx,shat))/eps
# 
# 
# 
# 
# 
```


So we can wrap this up in a function
```{r}
ebnm_normal_unknown_s = function(x,shat,z,mu=NULL,s2=NULL){
  fit = optim(par=0,fn=neg_loglik,x=x,shat=shat,z=z,mu=mu,s2=s2,method="Brent",lower=-5,upper=5)
  sb = exp(fit$par)
  aux = compute_w_mu_s2(sb,x,shat,z,mu,s2)
  return(list(sb=sb, s = sqrt(aux$s2), mu = aux$mu, loglik = -fit$value))  
}
fit = ebnm_normal_unknown_s(TT,shat,z)
```

# Posterior mean for $b$

The posterior mean, in the case $\mu=0$ can be shown to b:
$$E(b) = V diag(s_b^2 d_j^2/(1+s_b^2 d_j^2)) (T-\mu V'1_p) + \mu V'1_p$$
```{r}
shrink = fit$sb^2 * X.svd$d^2
shrink = shrink/(1+shrink)

post_mean = X.svd$v %*% (shrink*(TT-fit$mu*colSums(X.svd$v))) + fit$mu
plot(Y)
lines(X %*% btrue)
lines(X %*% post_mean,col=2)

```


## Iterative ridge

Try the iterative approach

```{r}
eb_ridge = function(y,X,mu=NULL,s2=NULL){
  X.svd = svd(X)
  TT = (t(X.svd$u) %*% Y)/X.svd$d # what i called T above
  z = colSums(X.svd$v)  
  shat = 1/X.svd$d
  fit = ebnm_normal_unknown_s(TT,shat,z,mu,s2)
  
  shrink = fit$sb^2 * X.svd$d^2
  shrink = shrink/(1+shrink)

  post_mean = X.svd$v %*% (shrink*(TT-fit$mu*colSums(X.svd$v))) + fit$mu
  return(list(fit=fit,post_mean=post_mean))
}

fit.eb = eb_ridge(Y,X)
Xnew = t(t(X)*as.vector(fit.eb$post_mean))
plot(Y)
lines(Xnew %*% rep(1,100))
fit.eb2 = eb_ridge(Y,Xnew)
lines(Xnew %*% fit.eb2$post_mean,col=2)
fit.eb2$fit
exp(-5)
```

So it seems to have worked - the second set is converging to the limits
of the search space for sb.

Try a sparser case:
```{r}
set.seed(1)
btrue = rep(0,100)
btrue[50] = 5
Y = X %*% btrue + 0.2*rnorm(n)
plot(Y)
lines(X %*% btrue)

fit.eb = eb_ridge(Y,X,mu=0)
plot(Y)
lines(X %*% fit.eb$post_mean)
par(mfcol=c(3,3),  mai=rep(0.2,4))
Xnew=X
for(i in 1:9){
  Xnew = t(t(Xnew)*as.vector(fit.eb$post_mean))
  fit.eb = eb_ridge(Y,Xnew,mu=0)
  plot(Y)
  lines(Xnew %*% fit.eb$post_mean,col=i)
}
```


So it seems that even with mu=0 we get "overfitting". This should
probably have been expected.



Try fixing s2
```{r}
set.seed(1)
btrue = rep(0,100)
btrue[50] = 5
Y = X %*% btrue + 0.2*rnorm(n)
plot(Y)
lines(X %*% btrue)

fit.eb = eb_ridge(Y,X,mu=0,s2=0.2)
plot(Y)
lines(X %*% fit.eb$post_mean)
par(mfcol=c(3,3),  mai=rep(0.2,4))
Xnew=X
for(i in 1:9){
  Xnew = t(t(Xnew)*as.vector(fit.eb$post_mean))
  fit.eb = eb_ridge(Y,Xnew,mu=0,s2=0.2)
  plot(Y)
  lines(Xnew %*% fit.eb$post_mean,col=i)
}
```


Try fixing s2 but not mu
```{r}
set.seed(1)
btrue = rep(0,100)
btrue[50] = 5
Y = X %*% btrue + 0.2*rnorm(n)
plot(Y)
lines(X %*% btrue)

fit.eb = eb_ridge(Y,X,s2=0.2)
plot(Y)
lines(X %*% fit.eb$post_mean)
par(mfcol=c(3,3),  mai=rep(0.2,4))
Xnew=X
for(i in 1:9){
  Xnew = t(t(Xnew)*as.vector(fit.eb$post_mean))
  fit.eb = eb_ridge(Y,Xnew,s2=0.2)
  plot(Y)
  lines(Xnew %*% fit.eb$post_mean,col=i)
}
```

so just based on this it seems varying mu converges to a less sparse solution;
probably a worse solution but not clear.


## Unused Stuff

The rest of this is old text that may be wrong...

Looking at the terms that depend on $\mu$:
$$l(\mu) = -0.5s^2\mu^2 x_+' \Sigma^{-1}x_+ + s \mu x_+' \Sigma^{-1}y $$
Differentiating, setting to 0,  we can write
$$l'(\mu) = -s^2 \mu tr(\Sigma^{-1}x_+ x'_+) + str(\Sigma^{-1}yx'_+) $$

## log-likelihood for $b$

The log-likelihood for $b$ is:
$$l(b) = -0.5 \log(|2\pi I_n|) - (0.5/s^2) (y-Xb)'(y-Xb)$$
Which up to a constant is
$$l(b) = (1/s^2) [-0.5b'X'Xb + b'X'y]$$


## Posterior mean and variance

The posterior precision is prior precision plus the likelihood-based precision.
So posterior variance satisfies
$$Var(b|Y) = [1/(s_b^2s^2) I_p+ (1/s^2)(X'X)]^{-1}$$
$$Var(b|Y) = s^2 s_b^2 (I_p+ s_b^2(X'X))^{-1}$$
The posterior mean for $b$ is:
$$\bar{b} = (X'X + (1/s^2_b) I_p)^{-1}(X'y + (1/s_b^2) s\mu 1_p)$$
or if we write $V$ for $\Var(b|Y)$ then
$$\bar{b} = (1/s^2) V(X'y+(1/s_b^2)s\mu1_p)$$


Alternatively, if we write in terms of $b/s$ we get
$$Var(b/s|Y) = s_b^2 (I_p+ s_b^2(X'X))^{-1}$$
$$E(b/s|Y) = (X'X + (1/s^2_b) I_p)^{-1}(X'(y/s) + (1/s_b^2) \mu 1_p)$$





# Expectation of $b'X'Xb$

We have
$$E(b'X'Xb) = E(tr(X'X bb'))  = tr(X'X E(bb'))$$ 
```{r}

```

