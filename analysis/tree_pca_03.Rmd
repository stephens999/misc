---
title: "tree_pca_03"
author: "Matthew Stephens"
date: "2020-08-08"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r}
library(flashr)
library(flashier)
library(magrittr)
library(sparsepca)
library(EbayesThresh)
```

## Introduction

The idea here is to look at behaviour of sparse PCA algorithms on
a simple tree.

It is a  tree with four tips and equal branch lengths. (Also no noise for now.)


```{r}
set.seed(123)
p = 1000
n = 20
f = list()
for(i in 1:6){ 
  f[[i]] = rnorm(p)
}
X =matrix(0,ncol=4*n, nrow=p)
X[,1:(2*n)] = f[[1]]
X[,(2*n+1):(4*n)] = f[[2]]

X[,1:n] = X[,1:n]+f[[3]]
X[,(n+1):(2*n)] = X[,(n+1):(2*n)]+f[[4]]
X[,(2*n+1):(3*n)] = X[,(2*n+1):(3*n)] + f[[5]]
X[,(3*n+1):(4*n)] = X[,(3*n+1):(4*n)] + f[[6]]
image(cor(X))
```


## Regular SVD

Regular SVD does not reproduce the tree here. Indeed we should not expect it to, because the third
and fourth eigenvectors have very similar eigenvalues which makes them
non-identifiable without sparsity:
```{r}
X.svd = svd(X)
X.svd$d[1:4]
par(mfcol=c(2,2))
plot(X.svd$v[,1])
plot(X.svd$v[,2])
plot(X.svd$v[,3])
plot(X.svd$v[,4])
```

## sparse PCA in `sparsepca` package

Try sparse PCA with default settings. 
It does pretty well. Maybe not as sparse as one would like.
```{r}
X.spca = spca(X,10)
par(mfcol=c(2,2))
plot(X.spca$loadings[,1])
plot(X.spca$loadings[,2])
plot(X.spca$loadings[,3])
plot(X.spca$loadings[,4])
```

Try increasing sparsity by increasing alpha. That lost the tree... too sparse! 
```{r}
X.spca = spca(X,10,alpha=0.01)
par(mfcol=c(2,2))
plot(X.spca$loadings[,1])
plot(X.spca$loadings[,2])
plot(X.spca$loadings[,3])
plot(X.spca$loadings[,4])
```

Try again.. .also not what I was hoping for.
```{r}
X.spca = spca(X,10,alpha=0.001)
par(mfcol=c(2,2))
plot(X.spca$loadings[,1])
plot(X.spca$loadings[,2])
plot(X.spca$loadings[,3])
plot(X.spca$loadings[,4])
```

## flash

Here I try flash. (Note that setting var_type has an effect; may want to look into that more, but for now i set it constant...).

The way I have the matrix set up, the columns (not the rows) 
are the individuals, so
for a drift model the "loadings"  here should be normal; for simplicity I just set them to point normal and hope it learns them to be normal.
```{r}
X.flash = flashr::flash(X,10,ebnm_fn = list(l="ebnm_pn", f="ebnm_pn"),var_type = "constant")
par(mfcol=c(2,2))
plot(X.flash$ldf$f[,1])
plot(X.flash$ldf$f[,2])
plot(X.flash$ldf$f[,3])
plot(X.flash$ldf$f[,4])
```

See if point laplace prior makes a difference. But it is basically indistinguishable.
```{r}
X.flash = flashr::flash(X,10,ebnm_fn = list(l="ebnm_pn", f="ebnm_pl"),var_type = "constant")
par(mfcol=c(2,2))
plot(X.flash$ldf$f[,1])
plot(X.flash$ldf$f[,2])
plot(X.flash$ldf$f[,3])
plot(X.flash$ldf$f[,4])
```

Try ash prior...but it looks about the same.
```{r}
X.flash = flashr::flash(X,10,ebnm_fn = list(l="ebnm_pn", f="ebnm_ash"),var_type = "constant")
par(mfcol=c(2,2))
plot(X.flash$ldf$f[,1])
plot(X.flash$ldf$f[,2])
plot(X.flash$ldf$f[,3])
plot(X.flash$ldf$f[,4])
```

In fact these all look essentially the same as the svd solution...
```{r}
plot(X.flash$ldf$f[,1],X.svd$v[,1])
plot(X.flash$ldf$f[,2],X.svd$v[,2])
plot(X.flash$ldf$f[,3],X.svd$v[,3])
plot(X.flash$ldf$f[,4],X.svd$v[,4])
```

I guess maybe at the initialization the prior gets estimated close to normal,
which results in no change....



## flashr::flashier point-laplace

Jason had some luck with point laplace prior, so I thought I would
add results with his code here. It did not seem to help.

```{r}
fl_pl <- flashier::flash.init(t(X)) %>%
  flashier::flash.set.verbose(0) %>%
  flashier::flash.add.greedy(Kmax = 4, 
                   prior.family = c(prior.point.laplace(), prior.normal())) %>%
  flashier::flash.backfit(tol = 1e-4)

for(i in 1:4){
  plot(fl_pl$loadings.pm[[1]][,i])
}
```

## Add noise

I tried adding some noise as I thought low noise could exacerbate convergence issues. I found sometimes it would help, depending on the seed.

Here's an example where it does not help;
```{r}
set.seed(9)
Xn = X + rnorm(4*n*p,sd=3)
Xn.flash = flashr::flash(Xn,10,ebnm_fn = list(l="ebnm_pn", f="ebnm_ash"),var_type = "constant")
par(mfcol=c(2,2))
plot(Xn.flash$ldf$f[,1])
plot(Xn.flash$ldf$f[,2])
plot(Xn.flash$ldf$f[,3])
plot(Xn.flash$ldf$f[,4])
```

Here is an example where it did help. (i had to search through several seeds to find one)
```{r}
set.seed(5)
Xn = X + rnorm(4*n*p,sd=3)
Xn.flash = flashr::flash(Xn,10,ebnm_fn = list(l="ebnm_pn", f="ebnm_ash"),var_type = "constant")
par(mfcol=c(2,2))
plot(Xn.flash$ldf$f[,1])
plot(Xn.flash$ldf$f[,2])
plot(Xn.flash$ldf$f[,3])
plot(Xn.flash$ldf$f[,4])
```

Is this because the svd happens to be sparse... looks like it. (That is, it is likely not really the noise per se that is helping here, but the initialization.)
```{r}
Xn.svd = svd(Xn)
Xn.svd$d[1:4]
par(mfcol=c(2,2))
plot(Xn.svd$v[,1])
plot(Xn.svd$v[,2])
plot(Xn.svd$v[,3])
plot(Xn.svd$v[,4])
```

Using this fit to initialize on the non-noisy data leads to a much better solution: 
```{r}
X.flash.warmstart = flashr::flash(X,K=4,f_init=Xn.flash,ebnm_fn = list(l="ebnm_pn", f="ebnm_ash"),var_type = "constant",backfit = TRUE,greedy = FALSE)
par(mfcol=c(2,2))
plot(X.flash.warmstart$ldf$f[,1])
plot(X.flash.warmstart$ldf$f[,2])
plot(X.flash.warmstart$ldf$f[,3])
plot(X.flash.warmstart$ldf$f[,4])
```

And the objective with warmstart is much larger, demonstrating this
is a convergence problem rather than a fundamental problem with the
objective function: 
```{r}
X.flash$objective
X.flash.warmstart$objective
```

## Simplify and investigate further

To further simplify I'm going to remove the effects of the top branches
so we are just left with the "difficult" part of the problem.
(Actually distinguishing the top 2 solutions between (1,1,1,1) and (1,1,-1,-1)
vs (1,1,0,0) and (0,0,1,1) is also interesting, but we leave that for now.)

```{r}
X2 = X- X.svd$u[,1:2] %*% diag(X.svd$d[1:2]) %*% t(X.svd$v[,1:2])
X2.svd= svd(X2)
plot(X2.svd$v[,1])
```

Run flashr::flash. As we know, with defaults, it just gives the svd solution.
That is, it does not move from the initial value. (Note: I tried decreasing tol to 1e-14 and it did not change this result, just takes much longer to converge.)
```{r}
X2.flash = flashr::flash(X2,1,var_type="constant",ebnm_fn = list(l="ebnm_pn",f="ebnm_ash"))
plot(X2.flash$ldf$f[,1],X2.svd$v[,1])
```


Now I'm going to try to force a sparser solution by using 
a fixed (sparse)  prior for f, and normal prior for l (with standard deviation given by the diagonal of the svd).
```{r}
sd_grid = seq(0,1,length=20)
mean = rep(0,20)
pi = rep(1/20,20)
gf = ashr::normalmix(pi,mean,sd_grid)

gl = ashr::normalmix(c(1),c(0),c(sqrt(X2.svd$d[1])))

X2.flash = flashr::flash(X2,1,var_type="constant",ebnm_fn = list(l="ebnm_ash",f="ebnm_ash"),ebnm_param = list(l=list(g=gl,fixg=TRUE), f=list(g = gf,fixg=TRUE)))
                 
plot(X2.flash$fit$EF[,1],X2.svd$v[,1])
```

I'm not sure I'm doing that right, but it did not seem to work...it is shrinking
everything too uniformly?


### Try iterative thresholding

To help me understand, I try to implement a simple
iterative-thresholding algorithm to estimate a single sparse PC,
along the lines of [this paper](https://projecteuclid.org/euclid.aos/1368018173)

Remarkably this example seems to not only converge to a good solution,
but does so starting from near the "other" good solution. (Presumably this
is because the likelihood slightly favours the one it converges to?)

```{r}
soft_thresh = function(x,lambda=0.1){
  ifelse(abs(x)>lambda,sign(x)*(abs(x)-lambda),0)
}
normalize = function(x){x/sqrt(sum(x^2))}
S = cov(X2)
set.seed(1)
TT = rnorm(nrow(S))
TT = S %*% TT
TT = soft_thresh(TT)
TT = normalize(TT)
par(mfcol=c(3,3),mai=rep(0.3,4))
plot(TT,main="iteration = 1")
  
for(outer in 1:8){
  for(i in 1:20){
    TT = S %*% TT
    TT = soft_thresh(TT)
    TT = normalize(TT)
  }
  plot(TT,main = paste0("iteration = ",i*outer+1))
}
```

Try again with smaller lambda - does not work so well, showing that lambda matters (not suprising).
```{r}
set.seed(1)
TT = rnorm(nrow(S))
TT = S %*% TT
TT = soft_thresh(TT,lambda=.01)
TT = normalize(TT)
par(mfcol=c(3,3),mai=rep(0.3,4))
plot(TT,main="iteration = 1")
  
for(outer in 1:8){
  for(i in 1:200){
    TT = S %*% TT
    TT = soft_thresh(TT,lambda=.01)
    TT = normalize(TT)
  }
  plot(TT,main = paste0("iteration = ",i*outer+1))
}
```


### Try posterior mean shrinkage

Our EB approaches work with posterior means, so 
I wanted to repeat the above with the posterior mean shrinkage
operator under a point-laplace prior instead.
This is the posterior mean with 0.5 on each component and a=3.

```{r}
x = rnorm(100)
w = 1 # prior on non-zero component
a = 3 # scale parameter of Laplace component
sd = 0.1 # assumed standard deviations of "data"
plot(x,postmean.laplace(x,sd,w,a))
abline(a=0,b=1,col="red",lwd=2)
plot(x,x/postmean.laplace(x,sd,w,a),main="shrinkage factor")
```

```{r}
set.seed(1)
TT = rnorm(nrow(S))
TT = S %*% TT
TT = postmean.laplace(TT,sd,w,a)
TT = normalize(TT)
par(mfcol=c(3,3),mai=rep(0.3,4))
plot(TT,main="iteration = 1")
  
for(outer in 1:8){
  for(i in 1:200){
    TT = S %*% TT
    TT = postmean.laplace(TT,1,w,a)
    TT = normalize(TT)
  }
  plot(TT,main = paste0("iteration = ",i*outer+1))
}
```

Try a different starting point:
```{r}
set.seed(2)
TT = rnorm(nrow(S))
TT = S %*% TT
TT = postmean.laplace(TT,sd,w,a)
TT = normalize(TT)
par(mfcol=c(3,3),mai=rep(0.3,4))
plot(TT,main="iteration = 1")
  
for(outer in 1:8){
  for(i in 1:200){
    TT = S %*% TT
    TT = postmean.laplace(TT,1,w,a)
    TT = normalize(TT)
  }
  plot(TT,main = paste0("iteration = ",i*outer+1))
}
```


More thoughts... wondering whether the problem is partly the independence
assumption in the VB approximation. It seems the likelihood for the eigenvector is quite flat, so we should be able to get to the sparse solution quite easily.
But I guess the posterior on (u,v) is quite dependent on one another.
Maybe we can make a low-rank approximation to the posterior instead of
independence?
