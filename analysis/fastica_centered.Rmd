---
title: "fastica_centered"
author: "Matthew Stephens"
date: "2025-11-14"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

I found [previously](fastica_02.html) that the original fastICA algorithm seems to have trouble finding groups that are unbalanced. I believe that this is because the log-cosh objective function favours symmetry. ie maximizing or minimizing $\log \cosh (X'w)$ (subject to $w'w=1$, which implies ||X'w||= 1 if $XX'=I$) tends to find symmetric $X'w$.
I want to try to fix this using an intercept. That is,
maximize $\log \cosh (X'w + c)$ over both $w$ and $c$. 

I gave Gemini this problem and it gave me the following 
response and code.

"This adds a new parameter, $c$, that we also need to optimize. We can derive the update rules by taking the partial derivatives of the objective function (negentropy approximation) with respect to both $w$ and $c$ and setting them to zero (this is the Karush-Kuhn-Tucker or KKT-based approach).

This gives us two conditions to satisfy:

For $w$: $E[X g(w^TX + c)] - \lambda w = 0$

For $c$: $E[g(w^TX + c)] = 0$

Where $g$ is your contrast function ($\tanh$) and $\lambda$ is a scalar (Lagrange multiplier).

Condition (2) is new and very intuitive: it means the algorithm must find an intercept $c$ that "centers" the transformed projection $g(P)$ to have a mean of zero. This leads to a simple two-step update within each iteration:

Update c: First, for a fixed w, find the c that solves $E[g(w^TX + c)] = 0$.

Update w: Then, using that new c, perform the standard FastICA update for w."

```{r}
#' Single-unit FastICA update with an intercept term 'c'
#'
#' @param X The pre-processed (centered and whitened) data matrix (p x n).
#' @param w The current weight vector (p x 1).
#' @param c The current intercept scalar.
#' @param c_update_steps How many Newton steps to take for 'c' per iteration. 
#'                       1 is usually sufficient.
#' @return A list containing the updated 'w' (unnormalized) and 'c'.

fastica_r1update_wc = function(X, w, c, c_update_steps = 1) {
  
  # 1. Normalize w (as before)
  w = w / sqrt(sum(w^2))
  
  # --- Step 1: Update the intercept 'c' ---
  
  # Get the current projection *without* the intercept
  # P_current is an (n x 1) vector
  P_current = as.vector(t(X) %*% w) 
  
  # We want to find 'c' that solves E[g(P_current + c)] = 0
  # We use one (or more) Newton steps: c_new = c - f(c) / f'(c)
  
  for (i in 1:c_update_steps) {
    P_shifted = P_current + c
    
    # g(P_current + c)
    G_c = tanh(P_shifted) 
    
    # g'(P_current + c)
    G2_c = 1 - tanh(P_shifted)^2 
    
    # E[g(P_current + c)]
    mean_G = mean(G_c) 
    
    # E[g'(P_current + c)]
    mean_G2 = mean(G2_c) 
    
    # The Newton step (add epsilon for stability)
    c = c - mean_G / (mean_G2 + 1e-6)
  }
  
  # --- Step 2: Update the weight vector 'w' ---
  
  # Now we use the *updated* 'c' to update 'w'
  # P = w'X + c (using the new 'c' from Step 1)
  P = P_current + c 
  
  # g(P)
  G = tanh(P)
  
  # g'(P)
  G2 = 1 - tanh(P)^2
  
  # The FastICA update rule for 'w':
  # w_new = E[X * g(P)] - E[g'(P)] * w
  # (Note: We use sum(G2) as an estimate for E[g'(P)] * n)
  # (and X %*% G as an estimate for E[X*g(P)] * n)
  # The 'n' scaling factor cancels, as 'w' is normalized next iteration.
  w_new = X %*% G - sum(G2) * w 
  
  # 4. Return the updated vector and intercept
  return(list(w = w_new, c = c))
}

#'
#' This is the function that the "profiled" and "skew" update rules
#' are trying to find an extremum (max/min) of.
#'
#' @param X The pre-processed (centered and whitened) data matrix (p x n).
#' @param w The weight vector (p x 1).
#' @param c The intercept scalar.
#' @return The scalar value of the objective function.

compute_objective = function(X, w, c) {
  
  # 1. Normalize w to stay on the ||w||=1 constraint
  w = w / sqrt(sum(w^2))
  
  # 2. Calculate the projection
  P = as.vector(t(X) %*% w) # (n x 1) vector

  # 3. Create the shifted projection
  S = P + c
  
  # 5. Apply G(u) = log(cosh(u))
  G_S = log(cosh(S))
  
  # 6. Compute the expectation (as a sample mean)
  objective_value = mean(G_S)
  
  return(objective_value)
}
```

## Preprocessing code


```{r}
preprocess = function(X, n.comp=10){
  n <- nrow(X)
  p <- ncol(X)
  X <- scale(X, scale = FALSE)
  X <- t(X)
  
  ## This appears to be equivalant to X1 = t(svd(X)$v[,1:n.comp])       
  V <- X %*% t(X)/n
  s <- La.svd(V)
  D <- diag(c(1/sqrt(s$d)))
  K <- D %*% t(s$u)
  K <- matrix(K[1:n.comp, ], n.comp, p)
  X1 <- K %*% X
  return(X1)
}
```



## Simulate data: 3 small groups

Here I simulate 3 groups, with only 20 members each, which I previously found that fastICA had trouble with.
```{r}
K=3
p = 1000
n = 100
set.seed(1)
L = matrix(0,nrow=n,ncol=K)
for(i in 1:K){L[sample(1:n,20),i]=1}
FF = matrix(rnorm(p*K), nrow = p, ncol=K)

X = L %*% t(FF) + rnorm(n*p,0,0.01)
plot(X %*% FF[,1])
```

### centered fastICA (random start)

When I initialize these new updates with a random w, it does not pick out a single source:
```{r}
X1 = preprocess(X)
w = rnorm(nrow(X1))
c = 0 # this does not matter as c is updated first
res = list(w=w,c=c)
for(i in 1:100)
  res = fastica_r1update_wc(X1,res$w,res$c)
cor(L,t(X1) %*% res$w)
plot(as.vector(t(X1) %*% res$w))
res$c
compute_objective(X1,res$w,res$c)
```


### centered fastICA (true factor start)

Now I try initializing at (or close to) a true factor: it converges to the solution I wanted, and more importantly *it has a much higher objective* than then random start. This confirms that the changed objective function is having the desired affect.
```{r}
w = X1 %*% L[,1]
plot(t(X1) %*% w)

c = 0 # note this does not matter much because c gets updated first in the algorithm
res = list(w=w,c=c)
for(i in 1:100)
  res = fastica_r1update_wc(X1,res$w,res$c)
cor(L,t(X1) %*% res$w)
plot(as.vector(t(X1) %*% res$w))
compute_objective(X1,res$w,res$c)
```

### centered fastICA (many random starts)

Here I try with 100 random starts - there are lots of different results, and some of them reach an objective similar to that from the true start.
```{r}
obj = rep(0,100)
for(seed in 1:100){
  set.seed(seed)
  res = list(w = rnorm(nrow(X1)), c=0)
  for(i in 1:100)
    res = fastica_r1update_wc(X1,res$w,res$c)
  obj[seed] = compute_objective(X1,res$w,res$c)
}
plot(obj)
```

Here are the results for the seed achieving the max. It works!
```{r}
seed= which.max(obj)
set.seed(seed)
res = list(w = rnorm(nrow(X1)), c=0)
for(i in 1:100)
  res = fastica_r1update_wc(X1,res$w,res$c)
cor(L,t(X1) %*% res$w)
plot(as.vector(t(X1) %*% res$w))
max(obj)
```




### Joint updates (Ignore for now?)

After some more conversations with Gemini, we came up with another approach that uses the total derivative of the profiled objective $J(w, c(w))$ to update $w$. We also came up with an approximation for $\hat{c}(w)$ as the negative cubed root of $E(w^3)$. I am not sure that the update is correct, but Gemini gave me a new update rule for $w$ that includes a correction term accounting for the dependence of $c$ on $w$, and also using the approximation for $c$. I try this empirically here and include for completeness, but I think there are a number of issues and don't think this is right. (It could be useful to investigate the approximation for c and the new update separately rather than doing both at once as here.) Just a placeholder for the idea that maybe we should jointly update w,c rather than block updates?

```{r}
#'
#' This is the function that the "profiled" and "skew" update rules
#' are trying to find an extremum (max/min) of.
#'
#' @param X The pre-processed (centered and whitened) data matrix (p x n).
#' @param w The weight vector (p x 1).
#' @return The scalar value of the objective function.

compute_profiled_objective = function(X, w) {
  
  # 1. Normalize w to stay on the ||w||=1 constraint
  w = w / sqrt(sum(w^2))
  
  # 2. Calculate the projection
  P = as.vector(t(X) %*% w) # (n x 1) vector
  
  # 3. Approximate c(w) from skewness (same as in your update rule)
  skewness_estimate = mean(P^3)
  c_approx = - sign(skewness_estimate) * (abs(skewness_estimate))^(1/3)
  
  # 4. Create the shifted projection
  S = P + c_approx
  
  # 5. Apply G(u) = log(cosh(u))
  G_S = log(cosh(S))
  
  # 6. Compute the expectation (as a sample mean)
  objective_value = mean(G_S)
  
  return(objective_value)
}

#' Single-unit FastICA update using the total derivative of the
#' profiled objective J(w, c(w))
#'
#' @param X The pre-processed (centered and whitened) data matrix (p x n).
#' @param w The current weight vector (p x 1).
#' @return The updated, unnormalized weight vector 'w'.

fastica_r1update_profiled = function(X, w){
  
  n_samples = ncol(X)
  
  # 1. Normalize w
  w = w / sqrt(sum(w^2))
  
  # 2. Get projection and approximate c(w)
  P = as.vector(t(X) %*% w)
  skewness_estimate = mean(P^3)
  c_approx = - sign(skewness_estimate) * (abs(skewness_estimate))^(1/3)
  
  # 3. Create the shifted projection
  P_shifted = P + c_approx
  
  # 4. Calculate g(P) and g'(P)
  G = tanh(P_shifted)
  G2 = 1 - tanh(P_shifted)^2
  
  # 5. Calculate all the expectation terms
  # (Note: We use sums/matrix products for (n * E[...]))
  E_Xg = X %*% G             # n * E[X g(P)]
  E_g2 = sum(G2)             # n * E[g'(P)]
  
  # --- New terms required for the correction ---
  E_g = sum(G)               # n * E[g(P)]
  E_Xg2 = X %*% G2           # n * E[X g'(P)]
  
  # Calculate the scalar correction factor
  # (E_g / n) / (E_g2 / n) = E_g / E_g2
  # Add epsilon for stability
  correction_scalar = E_g / (E_g2 + 1e-6) 
  
  # 6. Calculate the new correction term vector
  correction_term = correction_scalar * E_Xg2
  
  # 7. Apply the full, new update rule
  # w_new = (E_Xg - correction_term) - E_g2 * w
  w_new = (E_Xg - correction_term) - E_g2 * w
  
  return(w_new)
}
```

```{r}
set.seed(1)
w = X1 %*% L[,1] + rnorm(nrow(X1),0,5)
plot(t(X1) %*% w)
init_val = t(X1) %*% w
init_val = init_val/sqrt(mean(init_val^2))

for(i in 1:100)
  w = fastica_r1update_profiled(X1,w)
cor(L,t(X1) %*% w)
plot(as.vector(t(X1) %*% w))
compute_profiled_objective(X1,w)
```

Try with more random start and several random seeds - there are lots of different results, and none of them reach as big a value of the objective. 
```{r}
obj = rep(0,100)
for(seed in 1:100){
  set.seed(seed)
  w = rnorm(nrow(X1))
  for(i in 1:100)
    w = fastica_r1update_profiled(X1,w)
  obj[seed] = compute_profiled_objective(X1,w)
}
plot(obj)
```

Here are the results for the seed achieving the max. It roughly combines two of the groups.
```{r}
seed= which.max(obj)
set.seed(seed)
w = rnorm(nrow(X1))
for(i in 1:100)
  w = fastica_r1update_profiled(X1,w)
cor(L,t(X1) %*% w)
plot(as.vector(t(X1) %*% w))
```

