---
title: "flashier_sla_text"
author: "Matthew Stephens"
date: "2023-10-17"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

I want to try running flashier (non-negative) on some text data and see what happens. It is also a chance to try out the flashier release to CRAN.

I tried running flashier on both the log1p transformed counts directly, and log1p transform of fitted
values from a topic model. Both produce somewhat promising results. It is hard to beat the log1p transform for simplicity and speed. 

```{r}
library(Matrix)
library(readr)
library(tm)
library(fastTopics)
library(flashier)
library(ebpmf)
library(RcppML)

sla <- read_csv("../../gsmash/data/SLA/SCC2016/Data/paperList.txt")
sla <- sla[!is.na(sla$abstract),]
sla$docnum = 1:nrow(sla)
datax = readRDS('../../gsmash/data/sla_full.rds')
dim(datax$data)
sum(datax$data==0)/prod(dim(datax$data))
datax$data = Matrix(datax$data,sparse = TRUE)
```

## Data filtering

filter out some documents: use top 60% longest ones as in Ke and Wang 2022.

```{r}
doc_to_use = order(rowSums(datax$data),decreasing = T)[1:round(nrow(datax$data)*0.6)]
mat = datax$data[doc_to_use,]
sla = sla[doc_to_use,]
samples = datax$samples
samples = lapply(samples, function(z){z[doc_to_use]})
```

Filter out words that appear in less than 5 documents. 
Note: if you don't do this you can still get real factors that capture
very rare words co-occuring. Eg two authors that are cited together.
If you are interested in those factors, no need to filter...
```{r}
word_to_use = which(colSums(mat>0)>4)
mat = mat[,word_to_use]
mat = Matrix(mat,sparse=TRUE)
```

I tried both the log1p transform on its own (no normalization for document size).

I also tried normalizing for document size, and different pseudocounts, (10, 1, 0.1, 0.01) where the first and last I expected to be too big/small (but in fact the results with 0.01 look quite reasonable in many ways).
Note that to keep things sparse I use log(1+X/c) where c is the pseudo-count.
```{r}
lmat = Matrix(log(mat+1),sparse=TRUE)

docsize = rowSums(mat)
s = docsize/mean(docsize)
lmat_s_10 = Matrix(log(0.1*mat/s+1),sparse=TRUE)
lmat_s_1 = Matrix(log(mat/s+1),sparse=TRUE)
lmat_s_01 = Matrix(log(10*mat/s+1),sparse=TRUE)
lmat_s_001 = Matrix(log(100*mat/s+1),sparse=TRUE)
```


In addition to the pseudocount, we also have to choose how to regularize the estimates of tau (column-wise precision). It turns out this can have
quite a bit effect on results. If tau is not regularized then typically some tau get very big (very small variance) and, intuitively, one is going to "overfit" some words. In the following I implement a rule of thumb based on Jason's work: I compute the standard deviation of the transformed data for a Poisson random variable of rate $\mu=4/n$. The 4 comes from the fact that we filtered words that occured in less than 4 documents, so this is a lower bound on the average $\mu$ for each word.
(I ignore variation in document size in this calculation). I think this rule of thumb could be justified as a realistic lower bound on the variance you would expect under a Poisson distribution for the data.
(There are reasons to believe that text data may be underdispersed relative to Poisson, but I will ignore this for now.)
```{r}
mhat = 4/nrow(lmat)
xx = rpois(1e7,mhat) # random poisson
S10 = sd(log(0.1*xx+1))
S1 = sd(log(xx+1)) # sd of log(X+1)
S01 = sd(log(10*xx+1)) # sd if log(10X+1)
S001 = sd(log(100*xx+1)) # sd if log(10X+1)
print(c(S10,S1,S01,S001))
```

## Fit log1p transformed data

I fit each of the four different pseudocounts here. For comparison I also looked at the maximum likelihood estimates (Frobenius norm minimization, which assumes constant column variances).
```{r}
set.seed(1)
fit.nn = flash(lmat,ebnm_fn = c(ebnm::ebnm_point_exponential,ebnm::ebnm_point_exponential),var_type=2,greedy_Kmax = 200, S=S1)

set.seed(1)
fit.nn.s.10 = flash(lmat_s_10,ebnm_fn = c(ebnm::ebnm_point_exponential,ebnm::ebnm_point_exponential),var_type=2,greedy_Kmax = 200, S=S10)
  
set.seed(1)
fit.nn.s.1 = flash(lmat_s_1,ebnm_fn = c(ebnm::ebnm_point_exponential,ebnm::ebnm_point_exponential),var_type=2,greedy_Kmax = 200, S=S1)

set.seed(1)
fit.nn.s.01 = flash(lmat_s_01,ebnm_fn = c(ebnm::ebnm_point_exponential,ebnm::ebnm_point_exponential),var_type=2,greedy_Kmax = 200, S=S01)

set.seed(1)
fit.nn.s.001 = flash(lmat_s_001,ebnm_fn = c(ebnm::ebnm_point_exponential,ebnm::ebnm_point_exponential),var_type=2,greedy_Kmax = 200, S=S001)

set.seed(1)
fit.nn.ml = nmf(lmat,k = 100)

set.seed(1)
fit.nn.ml.s.1 = nmf(lmat_s_1, k=100)


saveRDS(fit.nn,file='../output/fit.nn.rds')
saveRDS(fit.nn.s.10,file='../output/fit.nn.s.10.rds')
saveRDS(fit.nn.s.1,file='../output/fit.nn.s.1.rds')
saveRDS(fit.nn.s.01,file='../output/fit.nn.s.01.rds')
saveRDS(fit.nn.s.001,file='../output/fit.nn.s.001.rds')
```

Look at the keywords for each factor. We see that the flash fits capture more interesting keywords than the ml fits. Generally the flash keywords seem to make some sense for all levels of the pseudocount (although I had to drop the keyword threshold for large pseudocounts).

The ml fits capture a lot of "single-word" factors. It turns out that each factor is loaded on quite a lot of documents (not shown here). So what seems to be happening is that it chooses to fit single common words to explain lots of documents, rather than a small set of words to explain a small set of documents (which is perhaps what we want!)
```{r}
# sets keywords to NA if number of document membership 
# in the factor does not exceeed docfilter
get_keywords = function(fit,thresh = 2,docfilter=0){
  if("flash" %in% class(fit)){
    LL <- fit$L_pm
    FF = fit$F_pm
  }
  
  if("nmf" %in% class(fit)){ # deals with RcppML::nmf fit
    LL = fit@w
    FF = t(fit@d*fit@h) 
  }
  
  rownames(LL)<-1:nrow(LL)

  Lnorm = t(t(LL)/apply(LL,2,max))
  Fnorm = t(t(FF)*apply(LL,2,max))
  khat = apply(Lnorm,1,which.max)
  Lmax = apply(Lnorm,1,max)
  
  khat[Lmax<0.1] = 0
  keyw.nn =list()

  for(k in 1:ncol(Fnorm)){
     if(sum(Lnorm[,k]>0.5)> docfilter){
      key = Fnorm[,k]>log(thresh)
     
      keyw.nn[[k]] = (colnames(mat)[key])[order(Fnorm[key,k],decreasing = T)]
     } else { 
       keyw.nn[[k]] = NA
     }
  }
  return(keyw.nn)
}
print(get_keywords(fit.nn))
print(get_keywords(fit.nn.s.10,1.2)) #there are no keywords at the default threshold
print(get_keywords(fit.nn.s.1))
print(get_keywords(fit.nn.s.01))
print(get_keywords(fit.nn.s.001))
print(get_keywords(fit.nn.ml))
print(get_keywords(fit.nn.ml.s.1))
```

It turns out that the flash fit (with psedocount =1) is actually
a better fit by Frobenius norm than the maximum likelihood fit! Maybe
the greedy approach of flash is helping it to find better solutions?
In general these plots don't really show very close correspondence between the data and the fit. 
```{r}
  fv= fitted(fit.nn.s.10)
  sub = sample(1:length(fv),100000)
  plot(lmat_s_10[sub],fv[sub],main="flash fit (pseudocount 10)")
  
  fv= fitted(fit.nn.s.1)
  plot(lmat_s_1[sub],fv[sub],main="flash fit (pseudocount 1)")

  fv= fitted(fit.nn.s.01)
  plot(lmat_s_01[sub],fv[sub],main="flash fit (pseudocount 0.1)")
  
  fv= fitted(fit.nn.s.001)
  plot(lmat_s_001[sub],fv[sub],main="flash fit (pseudocount 0.01)")
  
  fv= fit.nn.ml@w %*% (fit.nn.ml.s.1@d*fit.nn.ml.s.1@h)
  plot(lmat_s_1[sub],fv[sub], main = "mle fit")
  
  mean((lmat_s_1-fit.nn.ml@w %*% (fit.nn.ml.s.1@d*fit.nn.ml.s.1@h))^2)
  mean((lmat_s_1-fitted(fit.nn.s.1))^2)
```


## Comparing the fits

It is hard to go through all the different keyword lists,
so I tried comparing fits pairwise. The idea is to focus on factors being found by one fit and not the other when trying to assess
 whether you prefer one fit or the other.

First I compare pseudocount 1 and 10:
```{r}
cc = cor(fit.nn.s.1$F_pm,fit.nn.s.10$F_pm)
sum(cc>0.9)
```

See which ones are fit-specific
```{r}
spec1 = apply(cc,1,max)<0.9
spec2 = apply(cc,2,max)<0.9
print(get_keywords(fit.nn.s.1)[spec1])
print(get_keywords(fit.nn.s.10,1.2)[spec2])
```

Here I compare the fits with lower pseudocounts.
```{r}
compare = function(fit1,fit2){
  cc = cor(fit1$F_pm,fit2$F_pm)
  spec1 = apply(cc,1,max)<0.9
  spec2 = apply(cc,2,max)<0.9
  print(get_keywords(fit1)[spec1])
  print(get_keywords(fit2)[spec2])
}
```

Pseudocount 1 vs 0.1:

```{r}
compare(fit.nn.s.1,fit.nn.s.01)
```

Pseudocount 0.1 vs 0.01. The 0.01 are not as bad as I expected.
```{r}
compare(fit.nn.s.01,fit.nn.s.001)
```


Overall the results for pseudocounts 0.01-1 look kind of reasonable...


## Looking at memberships

Looking at the non-zero memberships, it seems all four pseudo-counts result in similar overall levels of sparsity of $L$.
```{r}
hist_lnorm = function(fit,...){
  LL = fit$L_pm
  Lnorm = t(t(LL)/apply(LL,2,max))
  hist(Lnorm[Lnorm>0.01],...)
}
hist_lnorm(fit.nn.s.10,main="pseudocount=10",ylim=c(0,800),nclass=20)
hist_lnorm(fit.nn.s.1,main="pseudocount=1",ylim=c(0,800),nclass=20)
hist_lnorm(fit.nn.s.01,main="pseudocount=0.1",ylim=c(0,800),nclass=20)
hist_lnorm(fit.nn.s.001,main="pseudocount=0.01",ylim=c(0,800),nclass=20)
```

Here I threshold the normalized L values at 0.2 to get an idea of how many factors are present per document. All the documents are loaded on the first factor so the ones that load on only one factor can be thought of as not really being assigned to any topic.
```{r}
LL = fit.nn.s.01$L_pm
FF = fit.nn.s.01$F_pm
Lnorm = t(t(LL)/apply(LL,2,max))
Fnorm = t(t(FF)*apply(LL,2,max))

nfac = rowSums(Lnorm>0.2)
hist(nfac,breaks = seq(0.5,9.5,length=10))
```

Here I make an initial structure plot of the results.
```{r}
structure_plot_general = function(Lhat,Fhat,grouping,title=NULL,
                                  loadings_order = 'embed',
                                  print_plot=FALSE,
                                  seed=12345,
                                  n_samples = NULL,
                                  gap=40,
                                  std_L_method = 'sum_to_1',
                                  show_legend=TRUE,
                                  K = NULL
                                  ){
  set.seed(seed)
  #s       <- apply(Lhat,2,max)
  #Lhat    <-   t(t(Lhat) / s)

  if(is.null(n_samples)&all(loadings_order == "embed")){
    n_samples = 2000
  }

  if(std_L_method=='sum_to_1'){
    Lhat = Lhat/rowSums(Lhat)
  }
  if(std_L_method=='row_max_1'){
    Lhat = Lhat/c(apply(Lhat,1,max))
  }
  if(std_L_method=='col_max_1'){
    Lhat = apply(Lhat,2,function(z){z/max(z)})
  }
  if(std_L_method=='col_norm_1'){
    Lhat = apply(Lhat,2,function(z){z/norm(z,'2')})
  }
  
  if(!is.null(K)){
    Lhat = Lhat[,1:K]
    Fhat = Fhat[,1:K]
  }
  Fhat = matrix(1,nrow=3,ncol=ncol(Lhat))
  if(is.null(colnames(Lhat))){
    colnames(Lhat) <- paste0("k",1:ncol(Lhat))
  }
  fit_list     <- list(L = Lhat,F = Fhat)
  class(fit_list) <- c("multinom_topic_model_fit", "list")
  p <- structure_plot(fit_list,grouping = grouping,
                      loadings_order = loadings_order,
                      n = n_samples,gap = gap,verbose=F) +
    labs(y = "loading",color = "dim",fill = "dim") + ggtitle(title)
  if(!show_legend){
    p <- p + theme(legend.position="none")
  }
  if(print_plot){
    print(p)
  }
  return(p)
}
```

This is structure plot (with first common factor set to 0)
```{r}
Lnorm0=Lnorm
Fnorm0=Fnorm
Lnorm0[,1]=0
Fnorm0[,1]=0
structure_plot_general(Lnorm0,Fnorm0)
```

```{r}
structure_plot_general(Lnorm,Fnorm,std_L_method = "col_max_1")
```


### Repeat for smaller pseudocount


```{r}
LL = fit.nn.s.001$L_pm
FF = fit.nn.s.001$F_pm
Lnorm = t(t(LL)/apply(LL,2,max))
Fnorm = t(t(FF)*apply(LL,2,max))

nfac = rowSums(Lnorm>0.2)
hist(nfac,breaks = seq(0.5,9.5,length=10))
```

This is structure plot (with first common factor set to 0)
```{r}
Lnorm0=Lnorm
Fnorm0=Fnorm
Lnorm0[,1]=0
Fnorm0[,1]=0
structure_plot_general(Lnorm0,Fnorm0)
```

Here without making the columns sum to 1. It is interesting that the 
plot seems to make the memberships here look more "binary" than for the
larger pseudo-count. 
```{r}
structure_plot_general(Lnorm,Fnorm,std_L_method = "col_max_1")
```




## Thresholding factors

One thing I noticed is that some factors have a single document that is "driving" them (membership 1 in the normalized L), and no other document that has appreciable membership (say 0.5) even though several documents will have membership. For example, take topic 86 in the 01 fit. From the
keywords it looks like  "recommender system" factor, but also a "nearest neighbor" factor. It seems to be driven by a single document that has both those features.
```{r}
get_keywords(fit.nn.s.01)[86]
LL = fit.nn.s.01$L_pm
FF = fit.nn.s.01$F_pm
Lnorm = t(t(LL)/apply(LL,2,max))
Fnorm = t(t(FF)*apply(LL,2,max))
Lnorm0=Lnorm
Fnorm0=Fnorm
Lnorm0[,1]=0
Fnorm0[,1]=0

plot(Lnorm[,86])
order(Lnorm[,86],decreasing = TRUE)[1:4]
sla[1181,]$abstract
sla[1395,]$abstract
sla[1460,]$abstract
sla[1024,]$abstract
```
It seems that this factor is being "polluted" by the strongest single document - it is perhaps actually a "nearest neighbor" factor, not a "recommender system" factor.  

Here I look at some other factors that have a single outlying document to see what they look like
```{r}
which(colSums(Lnorm0>0.5)==1)
get_keywords(fit.nn.s.01)[colSums(Lnorm0>0.5)==1]
plot(Lnorm[,17])
order(Lnorm[,17],decreasing = TRUE)[1:4]
sla[1789,]$abstract
sla[475,]$abstract
sla[792,]$abstract
sla[1781,]$abstract
```

```{r}
plot(Lnorm[,69])
order(Lnorm[,69],decreasing = TRUE)[1:4]
sla[1867,]$abstract
sla[288,]$abstract
sla[1751,]$abstract
sla[1258,]$abstract
```

Generally speaking it seems that these factors are not very interpretable, and should perhaps be filtered out. That is what
motivated me to implement the 'docfilter' variable in the 'get_keywords" function. 
```{r}
print(get_keywords(fit.nn.s.1,docfilter = 1))
print(get_keywords(fit.nn.s.01,docfilter = 1))
print(get_keywords(fit.nn.s.001,docfilter = 1))
```

For the future: it seems worth looking at whether generalized binary priors on L might help with this, since they might help avoid this kind of fit. 
I also wonder whether document-specific variances could help model outlying documents better.




## Backfitting

I try backfitting one fit - it did not change things much.
(When i tried backfitting the results with pseudocount 0.1 I got an error.)
```{r}
fit.nn.s.1.2 = flash_backfit(fit.nn.s.1)
compare(fit.nn.s.1,fit.nn.s.1.2)
```


## Adding a factor

I was struck by the "mri" factor in the fit with pseudocount =0.01 that did not appear in the fit with 0.1. This factor makes a lot of sense so I thought maybe this is just a failure to "find" this factor in the fit with pseudocount = 0.1, rather than an indication of its absense in that data set. Here I confirm this by adding this factor and backfitting - the new factor is kept indicating that it improves the ELBO. 
```{r}
fit.nn.s.01.b = flash_factors_init(fit.nn.s.01,init = list(u = cbind(fit.nn.s.001$L_pm[,64]) ,d=cbind(c(1),drop=FALSE), v=cbind(fit.nn.s.001$F_pm[,64]) ))
fit.nn.s.01.b %>% flash_backfit(kset=109)
get_keywords(fit.nn.s.01.b)[109]
```

## Adding factors from 0.01 fit to 0.1 fit

I wondered how many of the differences are due to this kind of issue. So I tried adding all the factors from the 0.01 fit that did not appear in the original 0.1 fit and backfitting. I tried adding them all at once and backfitting but my initial attempt at that gave an error (I may not have done it correctly though), so here i add them one at a time. Most (but not all) are kept, indicating that maybe many of the differences between the runs are simply due to the runs finding different solutions, rather than due to differences in the structure present by pseudocount.
```{r}
fit1 = fit.nn.s.01
fit2 = fit.nn.s.001
cc = cor(fit1$F_pm,fit2$F_pm)
spec2 = which(apply(cc,2,max)<0.9)
fit.nn.s.01.2 = fit.nn.s.01
for(i in spec2){
  init = list(u = cbind(fit2$L_pm[,i]),d= diag(1, nrow=1), v = cbind(fit2$F_pm[,i]))
  fit.nn.s.01.2 <- flash_factors_init(fit.nn.s.01.2, init = init, ebnm_fn= ebnm_point_exponential) 
  fit.nn.s.01.2 <- flash_backfit(fit.nn.s.01.2,kset=fit.nn.s.01.2$n_factors)
}
fit.nn.s.01.2 <- flash_nullcheck(fit.nn.s.01.2)
get_keywords(fit.nn.s.01.2,docfilter = 1)
```


## Constant variance

I thought I would try constant variance flash to see what happens (no need to regularize tau this way). It turns out to fit a very large number of single word factors... I ran it with Kmax=200 and it fit all 200 factors. I do just 30 here to illustrate more quickly. You can see it reduces the mean squared error compared with the "maximum likelihood" perhaps suggesting the greedy approach helps find a better fit?
```{r}
set.seed(1)
fit.nn.s.v0 = flash(lmat_s_1,ebnm_fn = c(ebnm::ebnm_point_exponential,ebnm::ebnm_point_exponential),var_type=0,greedy_Kmax = 30)
print(get_keywords(fit.nn.s.v0))
mean((lmat_s_1-fitted(fit.nn.s.v0))^2)
```


## Topic model


Here I fit a topic model with k= 100; this yields a visually better fit to large values.
```{r}
fit_nmf_k100 = fit_poisson_nmf(mat,k=100,init.method="random")
fvals.nmf.k100 = fit_nmf_k100$L %*% t(fit_nmf_k100$F)
plot(mat[sub],fvals.nmf.k100[sub])
plot(log(1+mat[sub]),log(1+fvals.nmf.k100[sub]))
```

I tried fitting flash to the transform of the fitted values. The rationale here is to use topic modelling to "denoise" the data and then 
transform the denoised data. However, there are computational issues with this approach in general... it seems like it will not be tractible in general because it cannot exploit sparsity, which is essential for big datasets. The keywords seem promising. 
Maybe we should experiment some more(?)
```{r}
set.seed(1)
fit.nn.nmf.k100 = flash(log(fvals.nmf.k100+1),ebnm_fn = c(ebnm::ebnm_point_exponential,ebnm::ebnm_point_exponential),var_type=2,greedy_Kmax = 200,S=0.01)
plot(log(1+mat[sub]),fitted(fit.nn.nmf.k100)[sub])
print(get_keywords(fit.nn.nmf.k100))
```


```{r}
  fv= fitted(fit.nn.nmf.k100)
  sub = sample(1:length(fv),100000)
  plot(lmat_s_1[sub],fv[sub])
```



## Anscombe transform

This is a very brief look at the anscombe transformation for comparison:
```{r}
fit.nn.a = flash(sqrt(mat+3/8),ebnm_fn = c(ebnm::ebnm_point_exponential,ebnm::ebnm_point_exponential),var_type=2,greedy_Kmax = 200, S=0.01)
print(get_keywords(fit.nn.a))
```

```{r}
fv= fitted(fit.nn.a)
sub = sample(1:length(fv),100000)
plot(sqrt(mat+3/8)[sub],fv[sub])
```


