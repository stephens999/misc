---
title: "irls"
author: "Matthew Stephens"
date: "2024-02-17"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

I wanted to use IRLS to fit a simple logistic regression.
The following code does so for all columns of X simultaneously.
That is, it fits y ~ mu + b Xj for j= 1...p.

## Single vector x

I start by writing a function to fit a single vector x.
The code is based on the IRLS algorithm for logistic regression, as described in the book "Elements of Statistical Learning" by Hastie, Tibshirani and Friedman (2009), section 4.4.1, written in a way that it makes it easy to vectorize/repeat over the columns of a matrix X via matrix multiplications. (See below)

```{r}
logistic_IRLS_simple <- function(x, y, max_iter = 100, tolerance = 1e-6) {
  # Initialize coefficients
  mu <- 0
  beta <- 0
  converged <- FALSE
  
  for (iter in 1:max_iter) {
    eta <- mu + x * beta  # Linear predictor
    pi <- exp(eta) / (1 + exp(eta))  # Predicted probabilities

    # Weights for IRLS 
    w <- pi * (1 - pi)

    # Working response variable
    z <- eta + (y - pi) / (pi * (1 - pi)) 

    # Weighted least squares update
    #These are the elements of the X'X matrix (a b) (c d)
    a = sum(w)
    b = sum(w*x)
    c = sum(w*x)
    d = sum(w*x^2)
    
    wz = sum(w*z)
    wxz = sum(w*x*z)
    
    new_mu <- (d*wz - b*wxz) / (a*d - b*c)
    new_beta <- (-c*wz + a*wxz) /  (a*d - b*c)

    # Check for convergence
    if (all(abs(new_beta - beta) < tolerance)) {
      converged = TRUE
      break
    }

    beta <- new_beta
    mu <- new_mu
  }

  # Return fitted coefficients
  return(list(mu = mu, beta = beta, converged = converged, iter=iter))
}

```

Now I want to test the function with some simulated data.
Simulate a simple logistic regression:
```{r}
set.seed(1)
n <- 10000
x <- rnorm(n)
eta <- 10 + 2*x
pi <- exp(eta) / (1 + exp(eta))
y <- rbinom(n, 1, pi)
logistic_IRLS_simple(x, y)
glm(y~x, family = binomial)
```


# Matrix version

I now write a function that repeats the above for all columns of X simultaneously via vector/matrix operations.
```{r}
logistic_IRLS_simple_matrix <- function(X, y, max_iter = 100, tolerance = 1e-6) {
  # Initialize coefficients
  p <- ncol(X)
  mu <- rep(0,p)
  beta <- rep(0,p)
  converged <- FALSE
  
  for (iter in 1:max_iter) {
    eta <- t(mu + t(X) * beta)  # Linear predictor
    pi <- exp(eta) / (1 + exp(eta))  # Predicted probabilities

    # Weights for IRLS 
    w <- pi * (1 - pi)

    # Working response variable
    z <- eta + (y - pi) / (pi * (1 - pi)) 

    # Weighted least squares update
    #These are the elements of the X'X matrix (a b) (c d)
    a = colSums(w)
    b = colSums(w*X)
    c = b
    d = colSums(w*X^2)
    
    wz = colSums(w*z)
    wxz = colSums(w*X*z)
    
    new_mu <- (d*wz - b*wxz) / (a*d - b*c)
    new_beta <- (-c*wz + a*wxz) /  (a*d - b*c)

    # Check for convergence
    if (all(abs(new_beta - beta) < tolerance)) {
      converged = TRUE
      break
    }

    beta <- new_beta
    mu <- new_mu
  }

  # Return fitted coefficients
  return(list(mu = mu, beta = beta, converged = converged, iter=iter))
}
```


Now I test the function with some simulated data. It seems to work.
```{r}
set.seed(1)
n <- 10000
X <- cbind(rnorm(n), rnorm(n))
eta <- 10 + 2*X[,1] + 3*X[,2]
pi <- exp(eta) / (1 + exp(eta))
y <- rbinom(n, 1, pi)

glm(y~X[,1], family = binomial)
glm(y~X[,2], family = binomial)

logistic_IRLS_simple_matrix(X, y)
```

