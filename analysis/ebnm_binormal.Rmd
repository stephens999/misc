---
title: "ebnm_binormal"
author: "Matthew Stephens"
date: "2025-03-12"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r}
library(ashr)
```

## Introduction

I want to try implementing ebnm for
a simple bimodal prior, consisting of a mixture of
two normals, one with mean 0 and one with non-zero mean.

With this prior on $\theta$, and $x_i | \theta \sim N(\theta,s^2)$ we have the marginal likelihood
$$x_i \sim \pi_0 N(0,\lambda^2 s_0^2+s^2) + \pi_1 N(\lambda, \lambda^2 s_0^2+s^2)$$
where $\lambda$ is a scaling factor to be estimated. For now I fix
$\pi_0=\pi_1=0.5$ and $s_0$ to be smallish (it controls how bimodal this prior is).

Here I implement this marginal likelihood and its gradient (the latter obtained with the help of google AI).
```{r}
dbinormal = function (x,s,s0,lambda,log=TRUE){
  pi0 = 0.5
  pi1 = 0.5
  s2 = s^2
  s02 = s0^2
  l0 = dnorm(x,0,sqrt(lambda^2 * s02 + s2),log=TRUE)
  l1 = dnorm(x,lambda,sqrt(lambda^2 * s02 + s2),log=TRUE)
  logsum = log(pi0*exp(l0) + pi1*exp(l1))
 
  m = pmax(l0,l1)
  logsum = m + log(pi0*exp(l0-m) + pi1*exp(l1-m))
  if (log) return(sum(logsum))
  else return(exp(sum(logsum)))
}

# Numerical gradient calculation
numerical_grad_dbinormal <- function(x, s, s0, lambda, delta = 1e-6) {
  f_plus <- dbinormal(x, s, s0, lambda + delta)
  f_minus <- dbinormal(x, s, s0, lambda - delta)
  return((f_plus - f_minus) / (2 * delta))
}

# Analytical gradient calculation
analytical_grad_dbinormal <- function(x, s, s0, lambda) {
  pi0 = 0.5
  pi1 = 0.5
  s2 = s^2
  s02 = s0^2
  sigma_lambda_sq <- lambda^2 * s02 + s2

  l0 <- dnorm(x, 0, sqrt(sigma_lambda_sq), log=TRUE)
  l1 <- dnorm(x, lambda, sqrt(sigma_lambda_sq), log=TRUE)

  dl0_dlambda <- -lambda * s02 / sigma_lambda_sq + lambda * s02 * x^2 / (sigma_lambda_sq^2)
  dl1_dlambda <- (x - lambda - lambda*s02) / sigma_lambda_sq + (x - lambda)^2 * lambda * s02 / (sigma_lambda_sq)^2

  # stably compute w0 and w1
  m <- pmax(l0, l1) # Find the maximum of l0 and l1
  w0 <- pi0 * exp(l0 - m) / (pi0 * exp(l0 - m) + pi1 * exp(l1 - m)) # Stable w0
  w1 <- pi1 * exp(l1 - m) / (pi0 * exp(l0 - m) + pi1 * exp(l1 - m)) # Stable w1


  grad_logsum <- w0 * dl0_dlambda + w1 * dl1_dlambda
  return(sum(grad_logsum)) 
}


# Example usage and comparison
x <- c(0.5,1,2)
s <- 1
s0 <- 0.5
lambda <- 1

num_grad <- numerical_grad_dbinormal(x, s, s0, lambda)
ana_grad <- analytical_grad_dbinormal(x, s, s0, lambda)

cat("Numerical Gradient:", num_grad, "\n")
cat("Analytical Gradient:", ana_grad, "\n")
```

## Optimization using optim

Now I will try using optim to optimize this function.
First I simulate some data.
```{r}
# Simulate data
set.seed(1)
s = 1
s0 = 0.1
lambda = exp(4)

s2 = s^2
s02 = s0^2
n = 1000
x = c(rnorm(n,0,sqrt(lambda^2 * s02 + s2)),rnorm(n,lambda,sqrt(lambda^2 * s02 + s2)))
hist(x)
```

What I found is that it seems important to use a method that can be given bounds. (Note that (0,max(x)) are natural bounds). Eg Brents method, or L-BFGS-B. Using BFGS only works if you get the starting value right. (Possibly
it works if you initialize at the upper bound, but it is hard to be confident that this will work generally).
```{r}
objective_function <- function(lambda) {
  -dbinormal(x, s, s0, lambda) # Negative for minimization
}

gradient_function <- function(lambda) {
  -analytical_grad_dbinormal(x, s, s0, lambda) # Negative gradient for minimization
}

# optimization result initializing at true value
optim(par = lambda,
                      fn = objective_function,
                      gr = gradient_function,
                      method = "BFGS") # Using BFGS which uses gradient

#optim result initializing at 1
optim(par = 1,
                      fn = objective_function,
                      gr = gradient_function,
                      method = "BFGS") # Using BFGS which uses gradient

#optim result initializing at exp(3)
optim(par = exp(3),
                      fn = objective_function,
                      gr = gradient_function,
                      method = "BFGS") # Using BFGS which uses gradient

#optim result initializing at max(x)
optim(par = max(x),
                      fn = objective_function,
                      gr = gradient_function,
                      method = "BFGS") # Using BFGS which uses gradient


#optim result using L-BFGS-B
optim(par = 1,
                      fn = objective_function,
                      lower = 0, upper = max(x),
                      method = "L-BFGS-B") 

#optim result using Brent
optim(par = 1,
                      fn = objective_function,
                      lower = 0, upper = max(x),
                      method = "Brent") 

```

Here is a plot of the likelihood surface. You can see that if it starts
at lambda too small then the gradient is huge, which I believe causes it to overshoot to crazy large values of lambda in methods where there is no upper bound. Maybe initializing at the upper bound (max(x)) will solve this, but it seems safer to use the bounded methods.
```{r}
lambda = seq(1,100,length=100)
y = sapply(lambda,function(l) dbinormal(x,s,s0,l,log=TRUE))
plot(lambda,y,type="l")
plot(lambda,y,type="l",ylim=c(-50000,0))
```


Here I use optimize (which is the same as optim with method="Brent") to find the maximum likelihood estimate of lambda.
The following fixes s0=0.01. It may be worth investigating the idea of fixing s0 to be 0.01s to kind of fix the shrinkage behavior?

```{r}
ebnm_binormal = function(x,s){
  s0 = 0.01
  lambda = optimize(function(lambda){-dbinormal(x,s,s0,lambda,log=TRUE)},
              lower = 0, upper = max(x))$minimum
  g = ashr::normalmix(pi=c(0.5,0.5), mean=c(0,lambda), sd=c(lambda * s0,lambda * s0))
  postmean = ashr::postmean(g,ashr::set_data(x,s))
  postsd = ashr::postsd(g,ashr::set_data(x,s))
  return(list(g = g, posterior = data.frame(mean=postmean,sd=postsd)))
}

res = ebnm_binormal(x,s)
plot(x,res$posterior$mean)
```




