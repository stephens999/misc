---
title: "nmu_em"
author: "Matthew Stephens"
date: "2025-02-23"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

My goal is to fit a version of the non-negative matrix underapproximation using an EM algorithm. 

The model for a data matrix $A$ is:
$$A = uv' + b + e$$
where $b_{ij} \sim Exp(\lambda)$, $e_{ij} \sim N(0,sigma^2)$, and $u,v$ are non-negative vectors to be estimated. Alternatively we could write $A \sim N(uv' + b,\sigma^2)$. 

If $\sigma^2=0$ then the mle for $u,v$ (integrating out $b,e$) should be a feasible solution to the underapproximation problem. However, introducing $\sigma>0$ is useful because it allows us to implement an EM algorithm. If $sigma^2$ is very small (compared with $1/\lambda$) then this will approximately solve (a version of) the non-negative matrix underapproximation problem. On the other hand, if $1/\lambda$ is very small compared with $\sigma$ then it will be closer to regular NMF.

NOTE: We could potentially use this idea within flashier to put priors on $u$ and $v$. Also, for modelling a symmetric matrix $A$ we could instead fit $A \sim N(uu'+b, \sigma^2)$. 

The ELBO is
$$F(u,v,q)= E_q((-1/2\sigma^2)||A-uv'-b||_2^2) + D_{KL}(q,g)$$
where $g$ is the prior on $b$, 
$$g(b)=\lambda \exp(-\lambda b).$$
Here $q$ plays the role of the (approximate) posterior distribution on $b$.

Given $q$, the ELBO is minimized for $u,v$ by solving 
$$\min_{u,v} ||A-\bar{b} - uv'||_2^2.$$ Alternatively, any step that reduces this objective function will increase the ELBO. Here we will apply the truncated power method to achieve this.

Given $u,v$ the ELBO is minimized by for each $b_{ij}$ by solving
$$q(b) \propto g(b) \exp((-1/2\sigma^2)(x_{ij}-b)^2) \propto \exp((-1/2\sigma^2)[b^2-2(x_{ij}-\lambda \sigma^2)b])$$
where $x_{ij} = A_ij - u_i v_j$. This is a truncated normal distribution, 
$$  q(b_{ij}) = N_+(x_{ij}-\lambda \sigma^2, \sigma^2)$$.
Fortunately, the mean of this distribution is easily computed.

Note: if $\sigma$ is very small then the mean of this truncated normal looks close to $(x)_+$.
```{r}
  sigma=0.01  
  x = seq(-0.5,0.5,length=20)
  plot(x,truncnorm::etruncnorm(0,Inf,x-sigma^2,sigma))
```


## Implementation (symmetric A)


I'm going to try the symmetric case ($A$ symmetric; $u=v$). First I simulate some non-negative data for testing. A is a block covariance matrix of 0s and 1s plus non-negative noise.
```{r}
set.seed(1)
n = 10
maxiter = 1000
x = cbind(c(rep(1,n),rep(0,n)), c(rep(0,n),rep(1,n)))
E = matrix(0.1*rexp(2*n*2*n),nrow=2*n)
E = E+t(E) #symmetric errors
A = x %*% t(x) + E
image(A)
```


I'm going to solve the symmetric nmf method by the thresholded power iteration, 
$$u \leftarrow (Au)_+$$
Note: this is not an algorithm I can find a reference for but I believe it is true that, on rescaling u to have unit norm, this iteration decreases $||A-uu'||$ subject to $u>0$ $||u||=1$. Then you can set $d=u'Au$ to minimize $||A-duu'||^2$.


```{r}
#truncate and normalize function
trunc_and_norm = function(u){
  uplus = ifelse(u>0,u,0)
  if(!all(uplus==0))
    uplus = uplus/sqrt(sum(uplus^2))
  return(uplus)
}

nmu_em = function(A, lambda=1, sigma=1, maxiter=100){
  b = matrix(0,nrow=nrow(A),ncol=ncol(A))

  # initialize u by svd (check both u and -u since either could work)
  u = svd(A)$u[,1]
  u1 = trunc_and_norm(u)
  u2 = trunc_and_norm(-u)
  if(t(u1) %*% A %*% u1 > t(u2) %*% A %*% u2){
    u = u1
  } else {
    u = u2
  }

  d = drop(t(u) %*% (A-b) %*% u)

  for(i in 1:maxiter){
    u = trunc_and_norm((A-b) %*% u)
    d = drop(t(u) %*% (A-b) %*% u)
    b = matrix(truncnorm::etruncnorm(a=0,mean= A-d*u %*% t(u)-lambda*sigma^2,sd=sigma),nrow=2*n)
  }

  d = drop(t(u) %*% (A-b) %*% u)
  return(list(u=u, d=d, b=b))
}

```

```{r}
fit = nmu_em(A, 1, 1)
image(fit$u %*% t(fit$u))
min(A- fit$d * fit$u %*% t(fit$u))
hist(A- fit$d * fit$u %*% t(fit$u))
```


```{r}
fit = nmu_em(A, 1, 0.1)
image(fit$u %*% t(fit$u))
min(A- fit$d * fit$u %*% t(fit$u))
hist(A- fit$d * fit$u %*% t(fit$u))
```

Note: if lambda is too small then the fit gets absorbed into u instead of b. This makes sense.
```{r}
fit = nmu_em(A, .1, 1)
image(fit$u %*% t(fit$u))
image(fit$b)
```

More generally, if lambda and sigma are not appropriate this is unlikely to work well:
```{r}
fit = nmu_em(A, 100, 100)
image(fit$u %*% t(fit$u))
image(fit$b)
```


Note: i did try applying this, accidentally, to a matrix where some A were negative, so there is no underapproximation solution. It still did something sensible!


## Estimating lambda and sigma

Note that
$$E(A-uv') = 1/\lambda$$ and 
$$E(A-uv')^2 = 1/\lambda^2 + \sigma^2$$.

So a method of moments suggests estimating $\lambda = 1/mean(A-uv')$
and $\sigma^2 = mean((A-uv')^2) - mean(A-uv')^2 = var(A-uv')$.

The following code implements this idea (repeats fitting nmu 5  times, re-estimating lambda and sigma using the above after each time).
Note that this is really just a first try. I think in practice, if we want an underapproximation, then we will want to do something to ensure that. Here I just initialize sigma to be small, but we need to think more about this, especially since we would like the method to be invariant to scaling of $A$.

One possibility would be to initialize sigma to be the estimate based on the first PC, 
`sigma= sqrt(mean((A - A.svd$d[1] * u %*% t(u))^2))`  
but maybe that is still too big since what we really want is that sigma is the residual when all of the (nonnegative) factors are taken out. Another possibility is to fix sigma by guessing that, say, the nonnegative factors will, in total, explain 99% of the variance. 

```{r}
nmu_em_estlambda = function(A, maxiter=100, lambda=1, sigma=0.1){
  b = matrix(0,nrow=nrow(A),ncol=ncol(A))

  # initialize u by svd (check both u and -u since either could work)
  A.svd = svd(A)
  u = A.svd$u[,1]
  
  u1 = trunc_and_norm(u)
  u2 = trunc_and_norm(-u)
  if(t(u1) %*% A %*% u1 > t(u2) %*% A %*% u2){
    u = u1
  } else {
    u = u2
  }

  d = drop(t(u) %*% (A-b) %*% u)
  
  for(j in 1:5){
    for(i in 1:maxiter){
      u = trunc_and_norm((A-b) %*% u)
      d = drop(t(u) %*% (A-b) %*% u)
      b = matrix(truncnorm::etruncnorm(a=0,mean= A-d*u %*% t(u)-lambda*sigma^2,sd=sigma),nrow=2*n)
    } 
    lambda = 1/mean(A-d * u %*% t(u))
    sigma = sd(A-d * u %*% t(u))
  }
  d = drop(t(u) %*% (A-b) %*% u)
  return(list(u=u, d=d, b=b, lambda=lambda, sigma=sigma))
}
```


Try it out:
```{r}
fit = nmu_em_estlambda(A)
fit$lambda
fit$sigma
image(fit$u %*% t(fit$u))
```


## A tree

Try a tree, and iteratively removing factors. We can see this gives something close to the result we want. 
```{r}
set.seed(1)
n = 40
x = cbind(c(rep(1,n),rep(0,n)), c(rep(0,n),rep(1,n)), c(rep(1,n/2),rep(0,3*n/2)), c(rep(0,n/2), rep(1,n/2), rep(0,n)), c(rep(0,n),rep(1,n/2),rep(0,n/2)), c(rep(0,3*n/2),rep(1,n/2)))
E = matrix(0.1*rexp(2*n*2*n),nrow=2*n)
E = E+t(E) #symmetric errors
A = x %*% t(x) + E
image(A)
fit = nmu_em_estlambda(A)
fit$lambda
fit$sigma
image(fit$u %*% t(fit$u))

A = A-fit$d*fit$u %*% t(fit$u)
fit = nmu_em_estlambda(A)
image(fit$u %*% t(fit$u))

A = A-fit$d*fit$u %*% t(fit$u)
fit = nmu_em_estlambda(A)
image(fit$u %*% t(fit$u))

A = A-fit$d*fit$u %*% t(fit$u)
fit = nmu_em_estlambda(A)
image(fit$u %*% t(fit$u))

A = A-fit$d*fit$u %*% t(fit$u)
fit = nmu_em_estlambda(A)
image(fit$u %*% t(fit$u))

A = A-fit$d*fit$u %*% t(fit$u)
fit = nmu_em_estlambda(A)
image(fit$u %*% t(fit$u))

A = A-fit$d*fit$u %*% t(fit$u)
fit = nmu_em_estlambda(A)
image(fit$u %*% t(fit$u))
```


## Further thoughts; scaling 

I'm not entirely satisfied with the way we estimate lambda,sigma - that needs some thought I think.

Also, I note that the result can
depend on the scale of $A,\lambda, \sigma$. We might want to frame the problem a bit differently to avoid that... eg by $A = \sigma(uv'+b+e)$ where $e \sim N(0,1)$ and $b \sim Exp(\lambda)$. This could help make the approach scale invariant -- that is, multiplying A by a constant would effectively not change the solution --  which seems desirable. (Another simpler way to do this would be to simply to scale A to have mean squared values of 1 before proceeding.)


We could try
$$A = \sigma(uv' + b + e)$$
where $u,v$ non-negative, $e \sim N(0,1)$ and $b\sim Exp(lambda)$.

The ELBO is
$$F(u,v,q)= E_q((-1/2\sigma^2)||A-\sigma uv'-\sigma b||_2^2) + D_{KL}(q,g)$$
where $g$ is the prior on $b$, 
$$g(b)=\lambda \exp(-\lambda b).$$
Here $q$ plays the role of the (approximate) posterior distribution on $b$.

Given $q$, this is minimized for $u,v$ by solving 
$$\min_{u,v} ||A/\sigma -\bar{b} - uv'||_2^2$$

Given $u,v$ this is minimized by for each $b_{ij}$ by solving
$$q(b) \propto g(b) \exp((-1/2)(x_{ij} - b)^2) \propto \exp((-1/2)[b^2-2(x_{ij}-\lambda)b])$$
where $x_{ij} = A_ij/\sigma - u_i v_j$. This is a truncated normal distribution, 
$$q(b_{ij}) = N_+(x_{ij}-\lambda, 1)$$.


